{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TEST HOME PAGE \u00b6 If you can read this, MkDocs is rendering content.","title":"Homelab Docs"},{"location":"#test-home-page","text":"If you can read this, MkDocs is rendering content.","title":"TEST HOME PAGE"},{"location":"cloudflare/cloudflare-tunnel-code-server/","text":"Secure Cloudflare Tunnel Access to Code-Server (pacard-sawmill) \u00b6 This document outlines the full configuration used to expose code-server on pacard-sawmill securely over the internet using Cloudflare Tunnel and Cloudflare Zero Trust . This ensures maximum security, no LAN exposure, and MFA-protected access from anywhere. Overview-2 \u00b6 This setup provides: Secure remote access to code-server Zero Trust identity protection (email + MFA) No VPN required No ports opened on your home network code-server bound only to localhost for safety Cloudflare Tunnel handles all public traffic Traffic flow: You \u2192 Cloudflare Zero Trust \u2192 Cloudflare Tunnel \u2192 pacard-sawmill \u2192 code-server LAN access is intentionally disabled. 1. Install cloudflared on pacard-sawmill \u00b6 curl -fsSL https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb -o cloudflared.deb sudo dpkg -i cloudflared.deb Authenticate the instance: cloudflared tunnel login Choose the domain osirispc.com . 2. Create the Tunnel \u00b6 cloudflared tunnel create pacard This creates credentials: /home/osirisortiz/.cloudflared/<UUID>.json 3. Configure the Tunnel \u00b6 Create and edit the config: sudo mkdir -p /etc/cloudflared sudo nano /etc/cloudflared/config.yml Paste: tunnel: pacard credentials-file: /home/osirisortiz/.cloudflared/<UUID>.json ingress: - hostname: code.osirispc.com service: http://127.0.0.1:8080 - service: http_status:404 Replace <UUID> accordingly. 4. Create DNS Route \u00b6 cloudflared tunnel route dns pacard code.osirispc.com This creates the necessary CNAME record. 5. Install and Start the Tunnel Service \u00b6 sudo cloudflared service install sudo systemctl enable cloudflared sudo systemctl start cloudflared Verify: systemctl status cloudflared Should show: Active (running) . 6. Secure with Cloudflare Zero Trust \u00b6 Navigate to: Cloudflare Dashboard \u2192 Zero Trust \u2192 Access \u2192 Applications \u2192 Add Application Select Self-hosted . Configure: Domain: code.osirispc.com Name: Code Server Access policy: Action: Allow Selector: Emails Value: osirisortizpc@gmail.com This requires identity login + MFA before accessing code-server. 7. Configure code-server to Bind Locally Only \u00b6 File: ~/.config/code-server/config.yaml Recommended: auth: none bind-addr: 127.0.0.1:8080 cert: false Restart code-server: pkill -f code-server code-server 8. Final Behavior Summary \u00b6 External access (WORKS): \u00b6 https://code.osirispc.com Local machine access (WORKS): \u00b6 http://127.0.0.1:8080 LAN access (BLOCKED intentionally): \u00b6 http://192.168.199.177:8080 This is the expected Zero Trust behavior. Optional: Enable LAN Access Later \u00b6 Change: bind-addr: 0.0.0.0:8080 Restart code-server. LAN devices will now reach the service. Result \u00b6 You now have: A secure, MFA-protected Cloudflare Tunnel A globally accessible development environment Zero exposure on your LAN A professional Zero Trust setup Fully isolated code-server endpoint This configuration is ideal for a hardened, flexible, secure homelab.","title":"Secure Cloudflare Tunnel Access to Code-Server (pacard-sawmill)"},{"location":"cloudflare/cloudflare-tunnel-code-server/#secure-cloudflare-tunnel-access-to-code-server-pacard-sawmill","text":"This document outlines the full configuration used to expose code-server on pacard-sawmill securely over the internet using Cloudflare Tunnel and Cloudflare Zero Trust . This ensures maximum security, no LAN exposure, and MFA-protected access from anywhere.","title":"Secure Cloudflare Tunnel Access to Code-Server (pacard-sawmill)"},{"location":"cloudflare/cloudflare-tunnel-code-server/#overview-2","text":"This setup provides: Secure remote access to code-server Zero Trust identity protection (email + MFA) No VPN required No ports opened on your home network code-server bound only to localhost for safety Cloudflare Tunnel handles all public traffic Traffic flow: You \u2192 Cloudflare Zero Trust \u2192 Cloudflare Tunnel \u2192 pacard-sawmill \u2192 code-server LAN access is intentionally disabled.","title":"Overview-2"},{"location":"cloudflare/cloudflare-tunnel-code-server/#1-install-cloudflared-on-pacard-sawmill","text":"curl -fsSL https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb -o cloudflared.deb sudo dpkg -i cloudflared.deb Authenticate the instance: cloudflared tunnel login Choose the domain osirispc.com .","title":"1. Install cloudflared on pacard-sawmill"},{"location":"cloudflare/cloudflare-tunnel-code-server/#2-create-the-tunnel","text":"cloudflared tunnel create pacard This creates credentials: /home/osirisortiz/.cloudflared/<UUID>.json","title":"2. Create the Tunnel"},{"location":"cloudflare/cloudflare-tunnel-code-server/#3-configure-the-tunnel","text":"Create and edit the config: sudo mkdir -p /etc/cloudflared sudo nano /etc/cloudflared/config.yml Paste: tunnel: pacard credentials-file: /home/osirisortiz/.cloudflared/<UUID>.json ingress: - hostname: code.osirispc.com service: http://127.0.0.1:8080 - service: http_status:404 Replace <UUID> accordingly.","title":"3. Configure the Tunnel"},{"location":"cloudflare/cloudflare-tunnel-code-server/#4-create-dns-route","text":"cloudflared tunnel route dns pacard code.osirispc.com This creates the necessary CNAME record.","title":"4. Create DNS Route"},{"location":"cloudflare/cloudflare-tunnel-code-server/#5-install-and-start-the-tunnel-service","text":"sudo cloudflared service install sudo systemctl enable cloudflared sudo systemctl start cloudflared Verify: systemctl status cloudflared Should show: Active (running) .","title":"5. Install and Start the Tunnel Service"},{"location":"cloudflare/cloudflare-tunnel-code-server/#6-secure-with-cloudflare-zero-trust","text":"Navigate to: Cloudflare Dashboard \u2192 Zero Trust \u2192 Access \u2192 Applications \u2192 Add Application Select Self-hosted . Configure: Domain: code.osirispc.com Name: Code Server Access policy: Action: Allow Selector: Emails Value: osirisortizpc@gmail.com This requires identity login + MFA before accessing code-server.","title":"6. Secure with Cloudflare Zero Trust"},{"location":"cloudflare/cloudflare-tunnel-code-server/#7-configure-code-server-to-bind-locally-only","text":"File: ~/.config/code-server/config.yaml Recommended: auth: none bind-addr: 127.0.0.1:8080 cert: false Restart code-server: pkill -f code-server code-server","title":"7. Configure code-server to Bind Locally Only"},{"location":"cloudflare/cloudflare-tunnel-code-server/#8-final-behavior-summary","text":"","title":"8. Final Behavior Summary"},{"location":"cloudflare/cloudflare-tunnel-code-server/#external-access-works","text":"https://code.osirispc.com","title":"External access (WORKS):"},{"location":"cloudflare/cloudflare-tunnel-code-server/#local-machine-access-works","text":"http://127.0.0.1:8080","title":"Local machine access (WORKS):"},{"location":"cloudflare/cloudflare-tunnel-code-server/#lan-access-blocked-intentionally","text":"http://192.168.199.177:8080 This is the expected Zero Trust behavior.","title":"LAN access (BLOCKED intentionally):"},{"location":"cloudflare/cloudflare-tunnel-code-server/#optional-enable-lan-access-later","text":"Change: bind-addr: 0.0.0.0:8080 Restart code-server. LAN devices will now reach the service.","title":"Optional: Enable LAN Access Later"},{"location":"cloudflare/cloudflare-tunnel-code-server/#result","text":"You now have: A secure, MFA-protected Cloudflare Tunnel A globally accessible development environment Zero exposure on your LAN A professional Zero Trust setup Fully isolated code-server endpoint This configuration is ideal for a hardened, flexible, secure homelab.","title":"Result"},{"location":"extra/briefings/","text":"Agent Briefings \u00b6 High-level summaries for future you, future bosses, and any other cleared personnel.","title":"Agent Briefings"},{"location":"extra/briefings/#agent-briefings","text":"High-level summaries for future you, future bosses, and any other cleared personnel.","title":"Agent Briefings"},{"location":"extra/owls/","text":"Owl Sightings \u00b6 Document all unusual network or system behavior here.","title":"Owl Sightings"},{"location":"extra/owls/#owl-sightings","text":"Document all unusual network or system behavior here.","title":"Owl Sightings"},{"location":"extra/red-room/","text":"Red Room Notes \u00b6 Initial log. Further entries to be filed by Agent Ortiz.","title":"Red Room Notes"},{"location":"extra/red-room/#red-room-notes","text":"Initial log. Further entries to be filed by Agent Ortiz.","title":"Red Room Notes"},{"location":"guides/mkdocs-dev-setup/","text":"\ud83c\udf32 1) How to Start the Dev Environment (Every Time) \u00b6 Whenever you want to edit your Homelab Docs, do this: \u2705 Step 1 \u2014 Go to your project \u00b6 cd ~/projects/homelab-docs \u2705 Step 2 \u2014 Activate your virtual environment \u00b6 source .venv/bin/activate You should see: (.venv) \u2705 Step 3 \u2014 Start MkDocs with live reload (the working mode) \u00b6 export MKDOCS_WATCH=1 mkdocs serve -a 0.0.0.0:8002 This gives you: Dynamic updates Polling mode (solved your code-server issue) Instant reload Smooth editing \ud83e\udded Your site is now live at: \u00b6 http://192.168.199.177:8002/ When you\u2019re done working: \u274c Stop the server \u00b6 Just press: CTRL + C Done. \ud83c\udf32 **2) Homelab Docs \u2013 MkDocs Dev Environment Setup \u00b6 This page explains how to start the development server for the Homelab Docs project. Following these steps ensures live reload, correct file watching, and a clean workflow. 1. Navigate to the Project Directory \u00b6 cd ~/projects/homelab-docs 2. **Activate the Python Virtual Environment \u00b6 source .venv/bin/activate Once activated, your prompt will show: (.venv) 3. Enable Polling Mode (Required for code-server) \u00b6 export MKDOCS_WATCH=1 Polling mode ensures MkDocs correctly detects file changes when using code-server. 4. Start the MkDocs Development Server \u00b6 mkdocs serve -a 0.0.0.0:8002 The site will become available at: http://192.168.199.177:8002/ 5. Stopping the Server \u00b6 Press: CTRL + C Notes \u00b6 Watch Setting in mkdocs.yml \u00b6 This project requires a manual watch directive so MkDocs always tracks the correct folder: watch: - docs Why We Use Polling Mode \u00b6 Because code-server writes files in a way that breaks standard file watchers, MKDOCS_WATCH=1 forces MkDocs to use a reliable, polling-based watcher that never misses changes. That\u2019s it. \u00b6 You now have a fully working development workflow for Homelab Docs.","title":"\ud83c\udf32 1) How to Start the Dev Environment (Every Time)"},{"location":"guides/mkdocs-dev-setup/#1-how-to-start-the-dev-environment-every-time","text":"Whenever you want to edit your Homelab Docs, do this:","title":"\ud83c\udf32 1) How to Start the Dev Environment (Every Time)"},{"location":"guides/mkdocs-dev-setup/#step-1-go-to-your-project","text":"cd ~/projects/homelab-docs","title":"\u2705 Step 1 \u2014 Go to your project"},{"location":"guides/mkdocs-dev-setup/#step-2-activate-your-virtual-environment","text":"source .venv/bin/activate You should see: (.venv)","title":"\u2705 Step 2 \u2014 Activate your virtual environment"},{"location":"guides/mkdocs-dev-setup/#step-3-start-mkdocs-with-live-reload-the-working-mode","text":"export MKDOCS_WATCH=1 mkdocs serve -a 0.0.0.0:8002 This gives you: Dynamic updates Polling mode (solved your code-server issue) Instant reload Smooth editing","title":"\u2705 Step 3 \u2014 Start MkDocs with live reload (the working mode)"},{"location":"guides/mkdocs-dev-setup/#your-site-is-now-live-at","text":"http://192.168.199.177:8002/ When you\u2019re done working:","title":"\ud83e\udded Your site is now live at:"},{"location":"guides/mkdocs-dev-setup/#stop-the-server","text":"Just press: CTRL + C Done.","title":"\u274c Stop the server"},{"location":"guides/mkdocs-dev-setup/#2-homelab-docs-mkdocs-dev-environment-setup","text":"This page explains how to start the development server for the Homelab Docs project. Following these steps ensures live reload, correct file watching, and a clean workflow.","title":"\ud83c\udf32 **2) Homelab Docs \u2013 MkDocs Dev Environment Setup"},{"location":"guides/mkdocs-dev-setup/#1-navigate-to-the-project-directory","text":"cd ~/projects/homelab-docs","title":"1. Navigate to the Project Directory"},{"location":"guides/mkdocs-dev-setup/#2-activate-the-python-virtual-environment","text":"source .venv/bin/activate Once activated, your prompt will show: (.venv)","title":"2. **Activate the Python Virtual Environment"},{"location":"guides/mkdocs-dev-setup/#3-enable-polling-mode-required-for-code-server","text":"export MKDOCS_WATCH=1 Polling mode ensures MkDocs correctly detects file changes when using code-server.","title":"3. Enable Polling Mode (Required for code-server)"},{"location":"guides/mkdocs-dev-setup/#4-start-the-mkdocs-development-server","text":"mkdocs serve -a 0.0.0.0:8002 The site will become available at: http://192.168.199.177:8002/","title":"4. Start the MkDocs Development Server"},{"location":"guides/mkdocs-dev-setup/#5-stopping-the-server","text":"Press: CTRL + C","title":"5. Stopping the Server"},{"location":"guides/mkdocs-dev-setup/#notes","text":"","title":"Notes"},{"location":"guides/mkdocs-dev-setup/#watch-setting-in-mkdocsyml","text":"This project requires a manual watch directive so MkDocs always tracks the correct folder: watch: - docs","title":"Watch Setting in mkdocs.yml"},{"location":"guides/mkdocs-dev-setup/#why-we-use-polling-mode","text":"Because code-server writes files in a way that breaks standard file watchers, MKDOCS_WATCH=1 forces MkDocs to use a reliable, polling-based watcher that never misses changes.","title":"Why We Use Polling Mode"},{"location":"guides/mkdocs-dev-setup/#thats-it","text":"You now have a fully working development workflow for Homelab Docs.","title":"That\u2019s it."},{"location":"infra/","text":"Infrastructure \u00b6 Welcome to the Infrastructure files \u2014 the wiring behind the curtains. Here you\u2019ll find the baseline maps, DNS field notes, storage strategy, and replication methods that keep the Lodges standing. Sections \u00b6 Overview Network Diagram Network Baseline DNS (Pi-hole + Unbound) Storage & Backups ZFS & VM Mirrors MkDocs Docker Instant Reload Fix","title":"Infrastructure Index"},{"location":"infra/#infrastructure","text":"Welcome to the Infrastructure files \u2014 the wiring behind the curtains. Here you\u2019ll find the baseline maps, DNS field notes, storage strategy, and replication methods that keep the Lodges standing.","title":"Infrastructure"},{"location":"infra/#sections","text":"Overview Network Diagram Network Baseline DNS (Pi-hole + Unbound) Storage & Backups ZFS & VM Mirrors MkDocs Docker Instant Reload Fix","title":"Sections"},{"location":"infra/dns-pihole-unbound/","text":"\ud83d\udcd8 Pi-hole + Unbound Deployment (Host Networking Mode) \u00b6 A Lodge Infrastructure Case File \ud83d\udcdd Overview \u00b6 This document describes the complete setup of Pi-hole with Unbound running on an Ubuntu 24.04 virtual machine using Docker. This configuration gives you: Network-wide ad blocking Full recursive DNS (no 3rd-party DNS) Local hostname resolution High privacy Fast DNS caching Simple failover options Pi-hole operates in host networking mode for maximum reliability, while Unbound runs in a standard Docker bridge network (exposing port 5335). \ud83c\udfdb\ufe0f Architecture \u00b6 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Lodge Network (LAN) \u2502 \u2502 Devices \u2192 192.168.199.53 (Pi-hole DNS Server) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Pi-hole (Docker, host networking) \u2502 \u2502 - Listens directly on host ports: 53, 80, 443, 123 \u2502 \u2502 - Forwards DNS to Unbound at 127.0.0.1#5335 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Unbound (Docker bridge) \u2502 \u2502 Listens on 0.0.0.0:5335 (TCP+UDP) \u2502 \u2502 Recursively queries the DNS root servers \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \ud83d\udda5\ufe0f VM Requirements \u00b6 Ubuntu Server 24.04 LTS Static IP: 192.168.199.53 1 CPU / 1\u20132GB RAM minimum Docker installed UFW disabled or configured to allow DNS (port 53) \ud83e\uddf0 Install Docker & Docker Compose \u00b6 sudo apt update sudo apt install docker.io docker-compose -y sudo systemctl enable --now docker \ud83d\udce6 Directory Structure \u00b6 Create the directory: mkdir -p /opt/pihole cd /opt/pihole \ud83e\uddfe Docker Compose File \u00b6 Create: sudo nano /opt/pihole/docker-compose.yml Paste: version: \"3.3\" services: pihole: container_name: pihole image: pihole/pihole:latest network_mode: host environment: TZ: \"America/New_York\" WEBPASSWORD: \"admin\" DNSMASQ_LISTENING: all volumes: - ./etc-pihole:/etc/pihole - ./etc-dnsmasq.d:/etc/dnsmasq.d restart: unless-stopped unbound: container_name: unbound image: mvance/unbound:latest ports: - \"5335:53/tcp\" - \"5335:53/udp\" restart: unless-stopped \u25b6\ufe0f Deployment \u00b6 Run: cd /opt/pihole sudo docker-compose down sudo docker-compose up -d --force-recreate Verify the containers: docker ps You should see: Pi-hole running and healthy Unbound running on port 5335 \ud83e\uddea Testing DNS \u00b6 Test recursion and forwarding: dig @192.168.199.53 google.com dig @192.168.199.53 cloudflare.com dig @192.168.199.53 nasa.gov Expected output: status: NOERROR ANSWER SECTION Times varying from 1ms (cached) to 100+ms (recursive) \u2699\ufe0f Pi-hole Configuration \u00b6 Go to: http://192.168.199.53/admin Upstream DNS \u00b6 Disable all upstream DNS providers. Add only: 127.0.0.1#5335 Advanced DNS settings \u00b6 Enable: \u2714 Listen on all interfaces \u2714 Never forward non-FQDNs \u2714 Never forward reverse lookups for private ranges \ud83c\udfe0 Optional: Conditional Forwarding \u00b6 To resolve local device names: Settings \u2192 DNS \u2192 Conditional Forwarding Local network: 192.168.199.0/24 Router IP: 192.168.199.1 Local domain name: lan \ud83d\udce1 Router Configuration \u00b6 Point LAN DNS to: 192.168.199.53 All clients will now use Pi-hole \u2192 Unbound \u2192 Root DNS servers. \ud83e\uddf0 Troubleshooting \u00b6 \u274c connection refused \u00b6 Cause: Pi-hole not running in host networking OR port 53 in use. Fix: Ensure network_mode: host is present Restart Pi-hole \u274c docker-proxy appears in ss output \u00b6 Cause: Pi-hole using ports: mapping instead of host networking. Fix: Remove ALL ports: from Pi-hole config Redeploy containers \u274c Unbound not responding \u00b6 Check port: dig @127.0.0.1 -p 5335 google.com If no response: Unbound container not ready Port blocked Container crashed \u274c Pi-hole not resolving LAN hostnames \u00b6 Enable Conditional Forwarding OR add host records in: /etc/pihole/custom.list \ud83e\udded Summary \u00b6 You now have: Full DNS privacy No 3rd-party upstream DNS Root-level recursion Fast caching Pi-hole running natively at host network speed A clean, secure, resilient Lodge DNS system. If you'd like an advanced version of this doc with: \ud83d\udd27 Hardening (DNSSEC, QNAME minimization, root anchors) \ud83d\udcca Grafana dashboards \ud83d\udd25 High availability (2nd Pi-hole) \ud83d\udce6 GitHub-ready SOP version \ud83d\udd3a Twin Peaks\u2013themed version Just say the word.","title":"DNS (Pi-hole + Unbound)"},{"location":"infra/dns-pihole-unbound/#pi-hole-unbound-deployment-host-networking-mode","text":"A Lodge Infrastructure Case File","title":"\ud83d\udcd8 Pi-hole + Unbound Deployment (Host Networking Mode)"},{"location":"infra/dns-pihole-unbound/#overview","text":"This document describes the complete setup of Pi-hole with Unbound running on an Ubuntu 24.04 virtual machine using Docker. This configuration gives you: Network-wide ad blocking Full recursive DNS (no 3rd-party DNS) Local hostname resolution High privacy Fast DNS caching Simple failover options Pi-hole operates in host networking mode for maximum reliability, while Unbound runs in a standard Docker bridge network (exposing port 5335).","title":"\ud83d\udcdd Overview"},{"location":"infra/dns-pihole-unbound/#architecture","text":"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Lodge Network (LAN) \u2502 \u2502 Devices \u2192 192.168.199.53 (Pi-hole DNS Server) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Pi-hole (Docker, host networking) \u2502 \u2502 - Listens directly on host ports: 53, 80, 443, 123 \u2502 \u2502 - Forwards DNS to Unbound at 127.0.0.1#5335 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u25bc \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Unbound (Docker bridge) \u2502 \u2502 Listens on 0.0.0.0:5335 (TCP+UDP) \u2502 \u2502 Recursively queries the DNS root servers \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518","title":"\ud83c\udfdb\ufe0f Architecture"},{"location":"infra/dns-pihole-unbound/#vm-requirements","text":"Ubuntu Server 24.04 LTS Static IP: 192.168.199.53 1 CPU / 1\u20132GB RAM minimum Docker installed UFW disabled or configured to allow DNS (port 53)","title":"\ud83d\udda5\ufe0f VM Requirements"},{"location":"infra/dns-pihole-unbound/#install-docker-docker-compose","text":"sudo apt update sudo apt install docker.io docker-compose -y sudo systemctl enable --now docker","title":"\ud83e\uddf0 Install Docker &amp; Docker Compose"},{"location":"infra/dns-pihole-unbound/#directory-structure","text":"Create the directory: mkdir -p /opt/pihole cd /opt/pihole","title":"\ud83d\udce6 Directory Structure"},{"location":"infra/dns-pihole-unbound/#docker-compose-file","text":"Create: sudo nano /opt/pihole/docker-compose.yml Paste: version: \"3.3\" services: pihole: container_name: pihole image: pihole/pihole:latest network_mode: host environment: TZ: \"America/New_York\" WEBPASSWORD: \"admin\" DNSMASQ_LISTENING: all volumes: - ./etc-pihole:/etc/pihole - ./etc-dnsmasq.d:/etc/dnsmasq.d restart: unless-stopped unbound: container_name: unbound image: mvance/unbound:latest ports: - \"5335:53/tcp\" - \"5335:53/udp\" restart: unless-stopped","title":"\ud83e\uddfe Docker Compose File"},{"location":"infra/dns-pihole-unbound/#deployment","text":"Run: cd /opt/pihole sudo docker-compose down sudo docker-compose up -d --force-recreate Verify the containers: docker ps You should see: Pi-hole running and healthy Unbound running on port 5335","title":"\u25b6\ufe0f Deployment"},{"location":"infra/dns-pihole-unbound/#testing-dns","text":"Test recursion and forwarding: dig @192.168.199.53 google.com dig @192.168.199.53 cloudflare.com dig @192.168.199.53 nasa.gov Expected output: status: NOERROR ANSWER SECTION Times varying from 1ms (cached) to 100+ms (recursive)","title":"\ud83e\uddea Testing DNS"},{"location":"infra/dns-pihole-unbound/#pi-hole-configuration","text":"Go to: http://192.168.199.53/admin","title":"\u2699\ufe0f Pi-hole Configuration"},{"location":"infra/dns-pihole-unbound/#upstream-dns","text":"Disable all upstream DNS providers. Add only: 127.0.0.1#5335","title":"Upstream DNS"},{"location":"infra/dns-pihole-unbound/#advanced-dns-settings","text":"Enable: \u2714 Listen on all interfaces \u2714 Never forward non-FQDNs \u2714 Never forward reverse lookups for private ranges","title":"Advanced DNS settings"},{"location":"infra/dns-pihole-unbound/#optional-conditional-forwarding","text":"To resolve local device names: Settings \u2192 DNS \u2192 Conditional Forwarding Local network: 192.168.199.0/24 Router IP: 192.168.199.1 Local domain name: lan","title":"\ud83c\udfe0 Optional: Conditional Forwarding"},{"location":"infra/dns-pihole-unbound/#router-configuration","text":"Point LAN DNS to: 192.168.199.53 All clients will now use Pi-hole \u2192 Unbound \u2192 Root DNS servers.","title":"\ud83d\udce1 Router Configuration"},{"location":"infra/dns-pihole-unbound/#troubleshooting","text":"","title":"\ud83e\uddf0 Troubleshooting"},{"location":"infra/dns-pihole-unbound/#connection-refused","text":"Cause: Pi-hole not running in host networking OR port 53 in use. Fix: Ensure network_mode: host is present Restart Pi-hole","title":"\u274c connection refused"},{"location":"infra/dns-pihole-unbound/#docker-proxy-appears-in-ss-output","text":"Cause: Pi-hole using ports: mapping instead of host networking. Fix: Remove ALL ports: from Pi-hole config Redeploy containers","title":"\u274c docker-proxy appears in ss output"},{"location":"infra/dns-pihole-unbound/#unbound-not-responding","text":"Check port: dig @127.0.0.1 -p 5335 google.com If no response: Unbound container not ready Port blocked Container crashed","title":"\u274c Unbound not responding"},{"location":"infra/dns-pihole-unbound/#pi-hole-not-resolving-lan-hostnames","text":"Enable Conditional Forwarding OR add host records in: /etc/pihole/custom.list","title":"\u274c Pi-hole not resolving LAN hostnames"},{"location":"infra/dns-pihole-unbound/#summary","text":"You now have: Full DNS privacy No 3rd-party upstream DNS Root-level recursion Fast caching Pi-hole running natively at host network speed A clean, secure, resilient Lodge DNS system. If you'd like an advanced version of this doc with: \ud83d\udd27 Hardening (DNSSEC, QNAME minimization, root anchors) \ud83d\udcca Grafana dashboards \ud83d\udd25 High availability (2nd Pi-hole) \ud83d\udce6 GitHub-ready SOP version \ud83d\udd3a Twin Peaks\u2013themed version Just say the word.","title":"\ud83e\udded Summary"},{"location":"infra/mkdocs-docker-instant-reload/","text":"Alright. Let\u2019s write it into your case file so Future-Osiris can pull it off the shelf and repeat it without a s\u00e9ance. Below is a ready-to-paste Markdown page plus the exact steps to place it and wire it into the sidebar. Step 1) Create the new doc page \u00b6 Machine: pacard-sawmill Path: /home/osirisortiz/projects/homelab-docs/docs/infra/mkdocs-docker-instant-reload.md Run: nano /home/osirisortiz/projects/homelab-docs/docs/infra/mkdocs-docker-instant-reload.md Paste this whole page: # MkDocs in Docker: Instant Reload Fix (Polling Watcher) When MkDocs runs inside a Docker container, it may not detect file changes from the host. Symptoms look like this: - You edit a `.md` file on the host - Browser shows old content - **Only updates after you restart the container** This happens because Docker bind mounts sometimes **do not forward file-change events** (inotify/watchdog) into the container. So MkDocs never gets the \u201csomething changed\u201d signal. The cure: **force polling** so MkDocs checks the filesystem on an interval. --- ## \u2705 The Golden Fix Add polling env vars and an explicit serve command to your Docker Compose service. ### Example `docker-compose.yml` ```yml services: mkdocs: image: squidfunk/mkdocs-material:9.5.31 container_name: mkdocs-dev ports: - \"8181:8000\" volumes: - /FULL/PATH/TO/YOUR/REPO:/docs environment: - WATCHFILES_FORCE_POLLING=true - WATCHFILES_POLL_DELAY_MS=300 command: serve -a 0.0.0.0:8000 --livereload -w docs -w mkdocs.yml restart: unless-stopped What each line is doing \u00b6 WATCHFILES_FORCE_POLLING=true Forces MkDocs\u2019 watcher to poll for changes instead of waiting for inotify events. WATCHFILES_POLL_DELAY_MS=300 How often it checks for changes (300ms feels instant without burning CPU). command: serve ... --livereload -w docs -w mkdocs.yml Makes the container run the dev server with: livereload enabled explicit watch targets so it always rebuilds on edits. volumes: /repo:/docs Always mount the whole repo folder , not individual files. Single-file mounts can \u201cfreeze\u201d because editors swap temp files/inodes. How to Verify It's Working \u00b6 Start the container: bash docker compose up -d --force-recreate docker logs -f mkdocs-dev You should see: Watching paths for changes: 'docs', 'mkdocs.yml' Serving on http://0.0.0.0:8000/ Edit any markdown file under docs/ and save. Logs should immediately show: Building documentation... Documentation built in ... If you see that without restarting the container, you're back on the good road. Notes \u00b6 MkDocs will warn about 0.0.0.0 being \u201cproduction-like.\u201d In Docker that\u2019s normal \u2014 it allows port mapping to your host. If your repo lives on a network share (NFS/CIFS), increase polling delay (ex: 500\u20131000ms). Quick Reference Snippet \u00b6 environment: - WATCHFILES_FORCE_POLLING=true - WATCHFILES_POLL_DELAY_MS=300 command: serve -a 0.0.0.0:8000 --livereload -w docs -w mkdocs.yml volumes: - /FULL/PATH/TO/REPO:/docs That\u2019s the whole spell. Save and exit: - **Ctrl+O**, Enter - **Ctrl+X** --- ## Step 2) Wire it into the sidebar (mkdocs.yml) **File:** `/home/osirisortiz/projects/homelab-docs/mkdocs.yml` Open it: ```bash nano /home/osirisortiz/projects/homelab-docs/mkdocs.yml Find your nav: section. Add this entry wherever your Infra/docker docs live. Example: nav: - Home: index.md - Infra: - Overview: infra/index.md - MkDocs Docker Instant Reload Fix: infra/mkdocs-docker-instant-reload.md If you already have an Infra section with other pages, just drop the new line under it. Save + exit. Step 3) Confirm it shows up \u00b6 If your homelab docs dev container is running on port 8002 , refresh: http://192.168.199.177:8002/ You should see a new sidebar item: Infra \u2192 MkDocs Docker Instant Reload Fix If you paste your current nav: block here, I\u2019ll tell you the exact spot to insert the line so it matches your structure perfectly.","title":"MkDocs Docker Instant Reload Fix"},{"location":"infra/mkdocs-docker-instant-reload/#step-1-create-the-new-doc-page","text":"Machine: pacard-sawmill Path: /home/osirisortiz/projects/homelab-docs/docs/infra/mkdocs-docker-instant-reload.md Run: nano /home/osirisortiz/projects/homelab-docs/docs/infra/mkdocs-docker-instant-reload.md Paste this whole page: # MkDocs in Docker: Instant Reload Fix (Polling Watcher) When MkDocs runs inside a Docker container, it may not detect file changes from the host. Symptoms look like this: - You edit a `.md` file on the host - Browser shows old content - **Only updates after you restart the container** This happens because Docker bind mounts sometimes **do not forward file-change events** (inotify/watchdog) into the container. So MkDocs never gets the \u201csomething changed\u201d signal. The cure: **force polling** so MkDocs checks the filesystem on an interval. --- ## \u2705 The Golden Fix Add polling env vars and an explicit serve command to your Docker Compose service. ### Example `docker-compose.yml` ```yml services: mkdocs: image: squidfunk/mkdocs-material:9.5.31 container_name: mkdocs-dev ports: - \"8181:8000\" volumes: - /FULL/PATH/TO/YOUR/REPO:/docs environment: - WATCHFILES_FORCE_POLLING=true - WATCHFILES_POLL_DELAY_MS=300 command: serve -a 0.0.0.0:8000 --livereload -w docs -w mkdocs.yml restart: unless-stopped","title":"Step 1) Create the new doc page"},{"location":"infra/mkdocs-docker-instant-reload/#what-each-line-is-doing","text":"WATCHFILES_FORCE_POLLING=true Forces MkDocs\u2019 watcher to poll for changes instead of waiting for inotify events. WATCHFILES_POLL_DELAY_MS=300 How often it checks for changes (300ms feels instant without burning CPU). command: serve ... --livereload -w docs -w mkdocs.yml Makes the container run the dev server with: livereload enabled explicit watch targets so it always rebuilds on edits. volumes: /repo:/docs Always mount the whole repo folder , not individual files. Single-file mounts can \u201cfreeze\u201d because editors swap temp files/inodes.","title":"What each line is doing"},{"location":"infra/mkdocs-docker-instant-reload/#how-to-verify-its-working","text":"Start the container: bash docker compose up -d --force-recreate docker logs -f mkdocs-dev You should see: Watching paths for changes: 'docs', 'mkdocs.yml' Serving on http://0.0.0.0:8000/ Edit any markdown file under docs/ and save. Logs should immediately show: Building documentation... Documentation built in ... If you see that without restarting the container, you're back on the good road.","title":"How to Verify It's Working"},{"location":"infra/mkdocs-docker-instant-reload/#notes","text":"MkDocs will warn about 0.0.0.0 being \u201cproduction-like.\u201d In Docker that\u2019s normal \u2014 it allows port mapping to your host. If your repo lives on a network share (NFS/CIFS), increase polling delay (ex: 500\u20131000ms).","title":"Notes"},{"location":"infra/mkdocs-docker-instant-reload/#quick-reference-snippet","text":"environment: - WATCHFILES_FORCE_POLLING=true - WATCHFILES_POLL_DELAY_MS=300 command: serve -a 0.0.0.0:8000 --livereload -w docs -w mkdocs.yml volumes: - /FULL/PATH/TO/REPO:/docs That\u2019s the whole spell. Save and exit: - **Ctrl+O**, Enter - **Ctrl+X** --- ## Step 2) Wire it into the sidebar (mkdocs.yml) **File:** `/home/osirisortiz/projects/homelab-docs/mkdocs.yml` Open it: ```bash nano /home/osirisortiz/projects/homelab-docs/mkdocs.yml Find your nav: section. Add this entry wherever your Infra/docker docs live. Example: nav: - Home: index.md - Infra: - Overview: infra/index.md - MkDocs Docker Instant Reload Fix: infra/mkdocs-docker-instant-reload.md If you already have an Infra section with other pages, just drop the new line under it. Save + exit.","title":"Quick Reference Snippet"},{"location":"infra/mkdocs-docker-instant-reload/#step-3-confirm-it-shows-up","text":"If your homelab docs dev container is running on port 8002 , refresh: http://192.168.199.177:8002/ You should see a new sidebar item: Infra \u2192 MkDocs Docker Instant Reload Fix If you paste your current nav: block here, I\u2019ll tell you the exact spot to insert the line so it matches your structure perfectly.","title":"Step 3) Confirm it shows up"},{"location":"infra/network-baseline/","text":"Network Baseline: Pi-hole 1 Configuration (Operational as of Nov 16, 2025) \u00b6 \ud83e\udde9 Overview \u00b6 Pi-hole 1 (192.168.199.53) is now operating as the primary DNS server for the network. The secondary DNS has been left as Cloudflare (1.1.1.1) temporarily for redundancy during observation. \ud83c\udf10 Router DHCP Configuration (TP-Link) \u00b6 DHCP Server: Enabled IP Address Pool: 192.168.199.99 \u2013 192.168.199.201 Address Lease Time: 120 minutes Default Gateway: 192.168.199.1 Primary DNS: 192.168.199.53 (Pi-hole 1) Secondary DNS: 1.1.1.1 (Cloudflare, temporary fallback) \ud83e\udde0 Pi-hole 1 Details \u00b6 Status: Running normally IP Address: 192.168.199.53 Role: Primary DNS server for LAN Container health: Healthy (FTL process active) DNS test results: Successful resolution via nslookup google.com 192.168.199.53 \ud83d\udd75\ufe0f Monitoring Instructions (48-hour observation) \u00b6 UI \u2192 Query Log: Confirm steady traffic and minimal \u201cretried\u201d queries. UI \u2192 Tools \u2192 Network: Verify active client list is populating. From any LAN host: bash nslookup cloudflare.com 192.168.199.53 Should return valid IPv4/IPv6 results quickly. If browsing stalls: bash pihole -t Use this to tail real-time logs and confirm query flow. \ud83e\udeb6 Next Steps (after stability confirmed) \u00b6 Once Pi-hole 1 proves stable: Deploy new Unbound server on a clean VM (not .139 ). Integrate Pi-hole 1 with that Unbound for recursive resolution. Remove Cloudflare fallback (1.1.1.1) to enforce full DNS filtering. \u2699\ufe0f Next Change Procedure: Switching to Recursive Unbound \u00b6 Deploy Unbound Container: bash docker run -d --name unbound \\ -p 5335:5335/tcp \\ -p 5335:5335/udp \\ -v /opt/unbound/etc:/etc/unbound \\ --restart unless-stopped \\ mvance/unbound:latest Update Pi-hole Upstream DNS in UI: Go to Settings \u2192 DNS \u2192 Upstream DNS Servers and set: Custom 1 (IPv4): 127.0.0.1#5335 Disable Cloudflare Fallback: Remove 1.1.1.1 from your router\u2019s DHCP config. Verify with dig/nslookup: bash dig google.com @127.0.0.1 -p 5335 If that resolves, Unbound is active. Reboot Clients or Flush DNS: bash ipconfig /flushdns # Windows sudo systemd-resolve --flush-caches # Linux Monitor Logs: bash docker logs -f unbound pihole -t Once Unbound has stable recursion, log the new baseline as Phase 2: Recursive DNS . \u2705 Current State Summary \u00b6 Component IP Address Role Status TP-Link Router 192.168.199.1 Gateway & DHCP Active Pi-hole 1 192.168.199.53 Primary DNS Operational Cloudflare 1.1.1.1 Secondary DNS (Fallback) Enabled Documented by Agent Osiris \u2013 maintained in the Lodge Systems Logbook","title":"Network Baseline"},{"location":"infra/network-baseline/#network-baseline-pi-hole-1-configuration-operational-as-of-nov-16-2025","text":"","title":"Network Baseline: Pi-hole 1 Configuration (Operational as of Nov 16, 2025)"},{"location":"infra/network-baseline/#overview","text":"Pi-hole 1 (192.168.199.53) is now operating as the primary DNS server for the network. The secondary DNS has been left as Cloudflare (1.1.1.1) temporarily for redundancy during observation.","title":"\ud83e\udde9 Overview"},{"location":"infra/network-baseline/#router-dhcp-configuration-tp-link","text":"DHCP Server: Enabled IP Address Pool: 192.168.199.99 \u2013 192.168.199.201 Address Lease Time: 120 minutes Default Gateway: 192.168.199.1 Primary DNS: 192.168.199.53 (Pi-hole 1) Secondary DNS: 1.1.1.1 (Cloudflare, temporary fallback)","title":"\ud83c\udf10 Router DHCP Configuration (TP-Link)"},{"location":"infra/network-baseline/#pi-hole-1-details","text":"Status: Running normally IP Address: 192.168.199.53 Role: Primary DNS server for LAN Container health: Healthy (FTL process active) DNS test results: Successful resolution via nslookup google.com 192.168.199.53","title":"\ud83e\udde0 Pi-hole 1 Details"},{"location":"infra/network-baseline/#monitoring-instructions-48-hour-observation","text":"UI \u2192 Query Log: Confirm steady traffic and minimal \u201cretried\u201d queries. UI \u2192 Tools \u2192 Network: Verify active client list is populating. From any LAN host: bash nslookup cloudflare.com 192.168.199.53 Should return valid IPv4/IPv6 results quickly. If browsing stalls: bash pihole -t Use this to tail real-time logs and confirm query flow.","title":"\ud83d\udd75\ufe0f Monitoring Instructions (48-hour observation)"},{"location":"infra/network-baseline/#next-steps-after-stability-confirmed","text":"Once Pi-hole 1 proves stable: Deploy new Unbound server on a clean VM (not .139 ). Integrate Pi-hole 1 with that Unbound for recursive resolution. Remove Cloudflare fallback (1.1.1.1) to enforce full DNS filtering.","title":"\ud83e\udeb6 Next Steps (after stability confirmed)"},{"location":"infra/network-baseline/#next-change-procedure-switching-to-recursive-unbound","text":"Deploy Unbound Container: bash docker run -d --name unbound \\ -p 5335:5335/tcp \\ -p 5335:5335/udp \\ -v /opt/unbound/etc:/etc/unbound \\ --restart unless-stopped \\ mvance/unbound:latest Update Pi-hole Upstream DNS in UI: Go to Settings \u2192 DNS \u2192 Upstream DNS Servers and set: Custom 1 (IPv4): 127.0.0.1#5335 Disable Cloudflare Fallback: Remove 1.1.1.1 from your router\u2019s DHCP config. Verify with dig/nslookup: bash dig google.com @127.0.0.1 -p 5335 If that resolves, Unbound is active. Reboot Clients or Flush DNS: bash ipconfig /flushdns # Windows sudo systemd-resolve --flush-caches # Linux Monitor Logs: bash docker logs -f unbound pihole -t Once Unbound has stable recursion, log the new baseline as Phase 2: Recursive DNS .","title":"\u2699\ufe0f Next Change Procedure: Switching to Recursive Unbound"},{"location":"infra/network-baseline/#current-state-summary","text":"Component IP Address Role Status TP-Link Router 192.168.199.1 Gateway & DHCP Active Pi-hole 1 192.168.199.53 Primary DNS Operational Cloudflare 1.1.1.1 Secondary DNS (Fallback) Enabled Documented by Agent Osiris \u2013 maintained in the Lodge Systems Logbook","title":"\u2705 Current State Summary"},{"location":"infra/network-diagram/","text":"EVIDENCE nav: - Home: index.md Infrastructure: Overview: infra/overview.md Network Diagram: infra/network-diagram.md Storage & Backups: infra/storage-backups.md Automation & Workflows: infra/automation.md Servers: Pacard Sawmill: servers/pacard-sawmill.md Gordon Cole: servers/gordon-cole.md Bookhouse: servers/bookhouse.md Blockbuster: servers/blockbuster.md RARR (Media Server): servers/rarr.md DNS Gateway (VM 198): servers/dns-gateway.md The White Lodge (PVE Node): servers/thewhitelodge.md The Black Lodge (PVE Node): servers/theblacklodge.md The Red Room (PVE Node / PBS): servers/theredroom.md Applications: Media: Jellyfin: apps/jellyfin.md Radarr: apps/radarr.md Sonarr: apps/sonarr.md Prowlarr: apps/prowlarr.md qBittorrent (VPN): apps/qbittorrent.md Xteve / TVHeadend: apps/xteve-tvheadend.md Books: Readarr: apps/readarr.md Kavita: apps/kavita.md Audiobookshelf: apps/audiobookshelf.md Networking & DNS: Pi-hole: apps/pihole.md Unbound: apps/unbound.md WireGuard: apps/wireguard.md Infrastructure Apps: Portainer: apps/portainer.md Nginx Proxy Manager: apps/npm.md Homepage Dashboard: apps/homepage.md Flame Dashboard: apps/flame.md Cloudflare: Tunnel for Code-Server: cloudflare/cloudflare-tunnel-code-server.md Future Tunnels: cloudflare/tunnels.md Guides: Container Deployment SOP: guides/container-deploy.md VM Creation SOP: guides/vm-create.md Network Troubleshooting: guides/network-troubleshoot.md Backup & Restore SOP: guides/backup-restore.md Important: Mermaid diagrams only work inside triple backticks ```mermaid \u2705 3. Make sure it\u2019s in the navigation (mkdocs.yml) \u00b6 You already have: ```yaml - Infrastructure: - Overview: infra/overview.md - Network Diagram: infra/network-diagram.md","title":"Network Diagram"},{"location":"infra/network-diagram/#3-make-sure-its-in-the-navigation-mkdocsyml","text":"You already have: ```yaml - Infrastructure: - Overview: infra/overview.md - Network Diagram: infra/network-diagram.md","title":"\u2705 3. Make sure it\u2019s in the navigation (mkdocs.yml)"},{"location":"infra/overview/","text":"Home: index.md Infrastructure: Overview: infra/overview.md Network Diagram: infra/network-diagram.md Storage & Backups: infra/storage-backups.md Automation & Workflows: infra/automation.md Servers: Pacard Sawmill: servers/pacard-sawmill.md Gordon Cole: servers/gordon-cole.md Bookhouse: servers/bookhouse.md Blockbuster: servers/blockbuster.md RARR (Media Server): servers/rarr.md DNS Gateway (VM 198): servers/dns-gateway.md The White Lodge (PVE Node): servers/thewhitelodge.md The Black Lodge (PVE Node): servers/theblacklodge.md The Red Room (PVE Node / PBS): servers/theredroom.md Applications: Media: Jellyfin: apps/jellyfin.md Radarr: apps/radarr.md Sonarr: apps/sonarr.md Prowlarr: apps/prowlarr.md qBittorrent (VPN): apps/qbittorrent.md Xteve / TVHeadend: apps/xteve-tvheadend.md Books: Readarr: apps/readarr.md Kavita: apps/kavita.md Audiobookshelf: apps/audiobookshelf.md Networking & DNS: Pi-hole: apps/pihole.md Unbound: apps/unbound.md WireGuard: apps/wireguard.md Infrastructure Apps: Portainer: apps/portainer.md Nginx Proxy Manager: apps/npm.md Homepage Dashboard: apps/homepage.md Flame Dashboard: apps/flame.md Cloudflare: Tunnel for Code-Server: cloudflare/cloudflare-tunnel-code-server.md Future Tunnels: cloudflare/tunnels.md Guides: Container Deployment SOP: guides/container-deploy.md VM Creation SOP: guides/vm-create.md Network Troubleshooting: guides/network-troubleshoot.md Backup & Restore SOP: guides/backup-restore.md","title":"Overview"},{"location":"lodges/","text":"The Lodges \u00b6 Field directory for each Lodge node and its purpose in the cascade. Nodes \u00b6 Packard Sawmill The White Lodge The Black Lodge The Owl Cave (NAS) The Bookhouse (Readarr VM) Gordon Cole Server RARR (Main Stack)","title":"Lodges Index"},{"location":"lodges/#the-lodges","text":"Field directory for each Lodge node and its purpose in the cascade.","title":"The Lodges"},{"location":"lodges/#nodes","text":"Packard Sawmill The White Lodge The Black Lodge The Owl Cave (NAS) The Bookhouse (Readarr VM) Gordon Cole Server RARR (Main Stack)","title":"Nodes"},{"location":"ops/","text":"Operations \u00b6 Daily procedures, tunnel work, failover plans, and other living runbooks. Runbooks \u00b6 Cloudflare Operations Code-Server Tunnel VPN Stack Automation Failover Cascade Model Jellyfin Infrastructure","title":"Operations Index"},{"location":"ops/#operations","text":"Daily procedures, tunnel work, failover plans, and other living runbooks.","title":"Operations"},{"location":"ops/#runbooks","text":"Cloudflare Operations Code-Server Tunnel VPN Stack Automation Failover Cascade Model Jellyfin Infrastructure","title":"Runbooks"},{"location":"servers/packard-sawmill/","text":"pacard-sawmill \u00b6 Role: Development server / code-server host OS: Ubuntu 24.04 LTS (VM) Primary IP: 192.168.199.177 Status: Active Overview \u00b6 pacard-sawmill serves as the main development environment for internal and external projects. It runs code-server, Docker, and several MkDocs documentation sites. Key Services \u00b6 code-server ( code.osirispc.com via Cloudflare Tunnel) Portainer Agent (port 9001) Homelab Docs ( homelab-docs auto-commit + watcher) Docker-based development containers Maintenance Notes \u00b6 Automatic updates via unattended-upgrades Auto-commit logs rotate every 50 lines Watched in real-time by homelab-autocommit.service Backups planned for sync to theowlcave NAS Recent Changes \u00b6 Last update: {{ date }}","title":"pacard-sawmill"},{"location":"servers/packard-sawmill/#pacard-sawmill","text":"Role: Development server / code-server host OS: Ubuntu 24.04 LTS (VM) Primary IP: 192.168.199.177 Status: Active","title":"pacard-sawmill"},{"location":"servers/packard-sawmill/#overview","text":"pacard-sawmill serves as the main development environment for internal and external projects. It runs code-server, Docker, and several MkDocs documentation sites.","title":"Overview"},{"location":"servers/packard-sawmill/#key-services","text":"code-server ( code.osirispc.com via Cloudflare Tunnel) Portainer Agent (port 9001) Homelab Docs ( homelab-docs auto-commit + watcher) Docker-based development containers","title":"Key Services"},{"location":"servers/packard-sawmill/#maintenance-notes","text":"Automatic updates via unattended-upgrades Auto-commit logs rotate every 50 lines Watched in real-time by homelab-autocommit.service Backups planned for sync to theowlcave NAS","title":"Maintenance Notes"},{"location":"servers/packard-sawmill/#recent-changes","text":"Last update: {{ date }}","title":"Recent Changes"},{"location":"servers/theblacklodge/","text":"theblacklodge \u00b6 Role: Secondary Proxmox Node (Backup / mirror media server) OS: Debian 12 (bookworm) Primary IP: 192.168.199.101 Status: Active (standby replication node) Overview \u00b6 The Black Lodge acts as the failover counterpart to The White Lodge. It mirrors media libraries, configurations, and snapshots via ZFS and rsync. When The White Lodge goes dark, The Black Lodge assumes its identity temporarily. Key Services \u00b6 Jellyfin (standby configuration) RARR stack (backup, disabled by default) Rsync mirror target for /mnt/data/movies and /mnt/data/shows Portainer Agent for container control Maintenance Notes \u00b6 Receives media and config sync via lodge-mirror.sh Keeps ZFS snapshots aligned daily Does not delete missing files (append-only sync) PBS handles local and offsite backups Failover Plan \u00b6 During failover, The Black Lodge mounts the mirrored datasets and starts the Jellyfin container. Cloudflare DNS may be updated manually or via Keepalived floating IP for continuity. Recent Changes \u00b6 Last update: {{ date }}","title":"theblacklodge"},{"location":"servers/theblacklodge/#theblacklodge","text":"Role: Secondary Proxmox Node (Backup / mirror media server) OS: Debian 12 (bookworm) Primary IP: 192.168.199.101 Status: Active (standby replication node)","title":"theblacklodge"},{"location":"servers/theblacklodge/#overview","text":"The Black Lodge acts as the failover counterpart to The White Lodge. It mirrors media libraries, configurations, and snapshots via ZFS and rsync. When The White Lodge goes dark, The Black Lodge assumes its identity temporarily.","title":"Overview"},{"location":"servers/theblacklodge/#key-services","text":"Jellyfin (standby configuration) RARR stack (backup, disabled by default) Rsync mirror target for /mnt/data/movies and /mnt/data/shows Portainer Agent for container control","title":"Key Services"},{"location":"servers/theblacklodge/#maintenance-notes","text":"Receives media and config sync via lodge-mirror.sh Keeps ZFS snapshots aligned daily Does not delete missing files (append-only sync) PBS handles local and offsite backups","title":"Maintenance Notes"},{"location":"servers/theblacklodge/#failover-plan","text":"During failover, The Black Lodge mounts the mirrored datasets and starts the Jellyfin container. Cloudflare DNS may be updated manually or via Keepalived floating IP for continuity.","title":"Failover Plan"},{"location":"servers/theblacklodge/#recent-changes","text":"Last update: {{ date }}","title":"Recent Changes"},{"location":"servers/thewhitelodge/","text":"thewhitelodge \u00b6 Role: Primary Proxmox Node OS: Debian 12 (bookworm) Primary IP: 192.168.199.100 Status: Active Overview \u00b6 The White Lodge hosts several vm's. The main RARR stack: Radarr, Sonarr, Jellyfin, qBittorrent, and Gluetun. It serves as the production media hub for the network and the primary sync source for The Black Lodge. Key Services \u00b6 Jellyfin (Movies, Shows, Trailers) Radarr, Sonarr, Prowlarr, Jellyseerr qBittorrent with VPN via Gluetun ZFS dataset for /mnt/data/movies and /mnt/data/shows Maintenance Notes \u00b6 ZFS snapshots replicate to The Black Lodge Synced nightly using lodge-mirror.sh Backed up via Proxmox Backup Server Config stored in /mnt/data/configs/ Failover Plan \u00b6 In the event of downtime, The Black Lodge takes over via manual ZFS replication and Jellyfin restore. Rsync ensures latest metadata and libraries are mirrored. Recent Changes \u00b6 Last update: {{ date }}","title":"thewhitelodge"},{"location":"servers/thewhitelodge/#thewhitelodge","text":"Role: Primary Proxmox Node OS: Debian 12 (bookworm) Primary IP: 192.168.199.100 Status: Active","title":"thewhitelodge"},{"location":"servers/thewhitelodge/#overview","text":"The White Lodge hosts several vm's. The main RARR stack: Radarr, Sonarr, Jellyfin, qBittorrent, and Gluetun. It serves as the production media hub for the network and the primary sync source for The Black Lodge.","title":"Overview"},{"location":"servers/thewhitelodge/#key-services","text":"Jellyfin (Movies, Shows, Trailers) Radarr, Sonarr, Prowlarr, Jellyseerr qBittorrent with VPN via Gluetun ZFS dataset for /mnt/data/movies and /mnt/data/shows","title":"Key Services"},{"location":"servers/thewhitelodge/#maintenance-notes","text":"ZFS snapshots replicate to The Black Lodge Synced nightly using lodge-mirror.sh Backed up via Proxmox Backup Server Config stored in /mnt/data/configs/","title":"Maintenance Notes"},{"location":"servers/thewhitelodge/#failover-plan","text":"In the event of downtime, The Black Lodge takes over via manual ZFS replication and Jellyfin restore. Rsync ensures latest metadata and libraries are mirrored.","title":"Failover Plan"},{"location":"servers/thewhitelodge/#recent-changes","text":"Last update: {{ date }}","title":"Recent Changes"}]}