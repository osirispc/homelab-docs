{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Homelab Overview","text":"<p>Welcome to the documentation for my homelab.</p> <p>This site is organized to give a clear view of the environment, the machines that power it, and the services that run on top. Use this page as your starting point and jump off into the deeper sections as needed.</p>"},{"location":"#goals-of-this-homelab","title":"Goals of This Homelab","text":"<ul> <li>Centralized media, services, and tooling</li> <li>A safe place to learn, test, and break things</li> <li>Practical experience with Linux, Proxmox, networking, and automation</li> <li>A documented setup that can be restored or rebuilt when something goes wrong</li> </ul>"},{"location":"#high-level-architecture","title":"High-Level Architecture","text":"<p>At a high level, the homelab consists of:</p> <ul> <li>Infrastructure</li> <li>Network (router, switches, VLANs, firewall rules)</li> <li>DNS and name resolution</li> <li>Storage (ZFS, datasets, shares)</li> <li> <p>Hypervisors (Proxmox nodes and virtualization layer)</p> </li> <li> <p>Servers</p> </li> <li>Physical hosts</li> <li>Proxmox nodes</li> <li> <p>Key VMs and containers</p> </li> <li> <p>Services</p> </li> <li>Media (e.g. Jellyfin)</li> <li>Remote access (VPN, Cloudflare Tunnel / Zero Trust)</li> <li>Developer tools (e.g. code-server)</li> <li> <p>Automation and scheduled jobs</p> </li> <li> <p>Guides &amp; Procedures</p> </li> <li>Step-by-step setup guides</li> <li>Troubleshooting notes</li> <li>Repeatable procedures (how to add a VM, how to restore a backup, etc.)</li> </ul>"},{"location":"#infrastructure-at-a-glance","title":"Infrastructure at a Glance","text":"<p>This section is documented in more detail under Infrastructure, but in summary:</p> <ul> <li>Network</li> <li>Core LAN and any lab VLANs</li> <li>Router / firewall</li> <li> <p>DNS (Pi-hole + Unbound or other resolvers)</p> </li> <li> <p>Storage</p> </li> <li>Main storage pool(s)</li> <li>Media datasets / shares</li> <li> <p>Backup locations and strategy</p> </li> <li> <p>Virtualization</p> </li> <li>Proxmox nodes</li> <li>How VMs and containers are generally laid out</li> </ul> <p>\u27a1\ufe0f See: Infrastructure section for details.</p>"},{"location":"#servers-at-a-glance","title":"Servers at a Glance","text":"<p>This section lists the main machines in the lab and what they are responsible for.</p> <p>Examples (adjust to match your actual hosts):</p> <ul> <li>Core Proxmox Node 1 \u2013 Primary hypervisor for most VMs</li> <li>Core Proxmox Node 2 \u2013 Secondary / backup node</li> <li>Storage / NAS \u2013 Holds main datasets and shares</li> <li>Utility / Toolbox VM(s) \u2013 For dev tools, scripts, and experiments</li> </ul> <p>Each server has (or will have) its own page under Servers describing:</p> <ul> <li>Role / purpose</li> <li>Hardware specs</li> <li>Important VMs/containers</li> <li>Backup and restore notes</li> <li>Access details</li> </ul> <p>\u27a1\ufe0f See: Servers section for details.</p>"},{"location":"#services-at-a-glance","title":"Services at a Glance","text":"<p>Key services running in the homelab include:</p> <ul> <li>Media</li> <li> <p>Jellyfin and related components</p> </li> <li> <p>Access</p> </li> <li>VPN for remote access</li> <li> <p>Cloudflare Tunnel / Zero Trust for secure external access</p> </li> <li> <p>Developer &amp; Utility</p> </li> <li>code-server and other tooling</li> <li> <p>Any utility dashboards or management interfaces</p> </li> <li> <p>Automation</p> </li> <li>Scheduled jobs</li> <li>Sync / failover scripts</li> <li>Any \u201ccascade\u201d or failover logic</li> </ul> <p>Each service has (or will have) its own page under Services explaining:</p> <ul> <li>What it does</li> <li>How it\u2019s deployed (VM, container, stack)</li> <li>Configuration notes</li> <li>How to troubleshoot or rebuild it</li> </ul> <p>\u27a1\ufe0f See: Services section for details.</p>"},{"location":"#documentation-layout","title":"Documentation Layout","text":"<p>This documentation is organized into the following main sections:</p> <ul> <li>Overview \u2013 This page. High-level summary and orientation.</li> <li>Infrastructure \u2013 Network, DNS, storage, hypervisors, and core platform.</li> <li>Servers \u2013 Individual physical hosts and important VMs.</li> <li>Services \u2013 Applications and services running in the lab.</li> <li>Guides \u2013 Setup guides, how-tos, and troubleshooting procedures.</li> <li>Operations (optional) \u2013 Monitoring, backup routines, failover, and maintenance.</li> <li>Reference / Case Files \u2013 Notes, experiments, and less formal material.</li> </ul> <p>As new systems are added or old ones are retired, this overview should be updated to reflect the current state of the homelab.</p> <p>Last updated: TODO-SET-DATE</p>"},{"location":"project-dashboard/","title":"Project dashboard","text":"<p>\ud83d\uddc2\ufe0f Project Dashboard</p> <p>A centralized index of all active development, documentation, and infrastructure projects on pacard-sawmill.</p> <p>\ud83d\udd25 Active Projects</p> <p>Homelab Docs (homelab-docs/)</p> <p>Your primary MkDocs documentation site for the Lodge infrastructure. Status: Active Notes: Nav cleanup currently underway.</p> <p>Work Docs (work-docs/)</p> <p>MkDocs environment for professional documentation at Boundless. Status: Active Next: Resolve container name conflict and redeploy mkdocs-work-dev.</p> <p>Trailer Downloader (trailer-downloader/)</p> <p>Automates Jellyfin preroll creation: select, process, and stitch trailers into a preroll reel. Status: Active Next: Add cron scheduling and .164 testing integration.</p> <p>Trailer Library (trailer-library/)</p> <p>Static repository for trailers, thumbnails, and metadata. Status: Stable Notes: Used by preroll generator.</p> <p>Budget Tracker App (budget_tracker_app/)</p> <p>Flutter-based budgeting and debt payoff app. Status: Under development Next: Implement data models and persistent storage.</p> <p>Chatwoot (chatwoot/)</p> <p>Production-ready deployment of Chatwoot with Docker. Status: Large, actively configured Next: Resume deploy/docker workflow and document usage.</p> <p>Red Lodge VPN (redlodge-vpn/)</p> <p>Cloudflare Tunnel + WireGuard overlay network for secure access. Status: Stable Next: Add key rotation + tunnel renewal documentation.</p> <p>RARR Stack (rarr-stack/)</p> <p>Radarr, Sonarr, qBittorrent, Gluetun, and automation stack. Status: Stable Next: Document integration with Lodge Cascade replication.</p> <p>\ud83c\udfa7 Creative / Media Projects</p> <p>Art Bell Project (artbell/)</p> <p>AI voice and audio production workflows inspired by Art Bell. Status: Ongoing creative development Next: Add automated episode generator script.</p> <p>UIR \u2013 Unusual Incident Reporter (uir/)</p> <p>iOS-based reporting tool (Swift) with PowerPoint adaptation. Status: Archived but important for portfolio Next: Add summary + screenshots.</p> <p>\ud83e\uddea Experimental / Utility Projects</p> <p>Black Lodge LLM (black-lodge-llm/)</p> <p>Local LLM environment for The Black Lodge node. Status: Early stage Next: Choose LLM backend (Ollama/LM Studio) and document setup.</p> <p>Termux Red Room Theme (termux-redroom-theme/)</p> <p>Custom theme/environment for Termux on Android. Status: Active Next: Add installer script and preview images.</p> <p>Servers (servers/)</p> <p>General server configuration templates. Status: Minimal Next: Merge into homelab-docs under \u201cThe Lodges.\u201d</p> <p>Network (network/)</p> <p>Network-related configs, templates, and notes. Status: Base structure Next: Integrate into \u201cInfrastructure\u201d section.</p> <p>Portainer (portainer/)</p> <p>Portainer + agent deployment templates. Status: Useful but simple Next: Move to /templates/docker/portainer.</p> <p>Home Assistant (homeassistant/)</p> <p>Early HA tests and experiments. Status: Inactive Next: Review for archival or rewrite.</p> <p>Knowledge Base (knowledge-base/)</p> <p>General knowledge notes. Status: Inactive Next: Merge into homelab-docs or archive.</p> <p>\ud83d\udce6 Archives / Backup</p> <p>redroom-merge.tar.gz</p> <p>Manual backup archive. Next: Move into /archives/redroom/.</p> <p>\ud83d\udccc Suggested Nav Entry</p> <p>Add this to your mkdocs.yml:</p> <ul> <li>Project Dashboard: project-dashboard.md</li> </ul> <p>If you want, Agent\u2026 I can install this page for you automatically, update the nav, or generate READMEs for each project.</p>"},{"location":"active-directory/","title":"Active Directory Overview","text":"<p>Welcome to the LodgeLab Active Directory documentation.</p> <p>This section covers the full lifecycle of your domain environment, including:</p> <ul> <li>Domain Controller deployment  </li> <li>OU and user structure planning  </li> <li>Group Policy design and testing  </li> <li>DNS integration  </li> <li>Health monitoring and maintenance  </li> </ul> <p>Use this index as the starting point for navigating the LodgeLab AD forest.</p>"},{"location":"active-directory/#key-components","title":"Key Components","text":"<ul> <li>Domain Controllers: Manage authentication, directory services, and replication  </li> <li>DNS (AD-Integrated): Provides name resolution for all domain services  </li> <li>Organizational Units: Logical grouping of users, computers, and policies  </li> <li>Group Policy: Centralized configuration and security enforcement  </li> <li>Certificate Services (optional): Issue and manage internal certificates  </li> </ul>"},{"location":"active-directory/#current-lodgelab-environment","title":"Current LodgeLab Environment","text":"<ul> <li>Forest Name: <code>thelorgelab.local</code></li> <li>Domain Functional Level: 2019 (default)</li> <li>DC Name: <code>DC-LAB</code></li> <li>SYSVOL: Healthy  </li> <li>DNS: Active and AD-integrated  </li> </ul> <p>More details can be found under:</p> <ul> <li>LodgeLab Domain Controller Build</li> <li>OU &amp; User Structure</li> <li>Group Policy</li> <li>DNS (AD Integrated)</li> <li>AD Health &amp; Tools</li> </ul>"},{"location":"active-directory/group-policy/","title":"Group Policy Framework","text":"<p>This document outlines the Group Policy strategy for the LodgeLab AD environment.</p>"},{"location":"active-directory/group-policy/#core-gpo-principles","title":"Core GPO Principles","text":"<ul> <li>Least privilege first</li> <li>Separate workstation and server policies</li> <li>Layered, modular GPO design</li> <li>Document every change</li> </ul>"},{"location":"active-directory/group-policy/#recommended-gpo-categories","title":"Recommended GPO Categories","text":""},{"location":"active-directory/group-policy/#1-baseline-security","title":"1. Baseline Security","text":"<ul> <li>Password policy  </li> <li>Account lockout  </li> <li>Audit policy  </li> <li>Screen timeout  </li> <li>Disable guest accounts  </li> </ul>"},{"location":"active-directory/group-policy/#2-workstation-policies","title":"2. Workstation Policies","text":"<ul> <li>Disable USB mass storage  </li> <li>Configure Windows Update behavior  </li> <li>Firewall policy  </li> <li>RDP restrictions  </li> </ul>"},{"location":"active-directory/group-policy/#3-server-policies","title":"3. Server Policies","text":"<ul> <li>Harden RDP  </li> <li>Restrict PowerShell execution  </li> <li>Disable unnecessary services  </li> <li>Configure event logging  </li> </ul>"},{"location":"active-directory/group-policy/#4-user-experience-policies","title":"4. User Experience Policies","text":"<ul> <li>Map network drives  </li> <li>Desktop shortcuts  </li> <li>Default browser settings  </li> <li>Login banner  </li> </ul>"},{"location":"active-directory/group-policy/#gpo-processing-order","title":"GPO Processing Order","text":"<ol> <li>Local</li> <li>Site</li> <li>Domain</li> <li>OU (deepest OU wins)</li> </ol>"},{"location":"active-directory/group-policy/#testing-gpos","title":"Testing GPOs","text":"<p>Use:</p> <p>```powershell gpupdate /force gpresult /r gpresult /h gp-report.html</p> <p>A joined Windows 10/11 client VM is recommended for validation.</p> <p>Next Steps</p> <p>Define baseline GPO settings</p> <p>Build workstation and server policies</p> <p>Attach GPOs to appropriate OUs</p>"},{"location":"active-directory/group-policy/#4-active-directorydnsmd","title":"\u2705 4. active-directory/dns.md","text":"<p>```markdown</p>"},{"location":"active-directory/group-policy/#active-directoryintegrated-dns","title":"Active Directory\u2013Integrated DNS","text":"<p>LodgeLab uses DNS integrated directly into Active Directory. This provides fast replication, automatic zone management, and secure updates.</p>"},{"location":"active-directory/group-policy/#key-zones","title":"Key Zones","text":"<p>After domain creation, two critical zones were generated:</p> <ul> <li><code>thelorgelab.local</code></li> <li><code>_msdcs.thelorgelab.local</code></li> </ul> <p>Both are AD-integrated and replicate via SYSVOL/DFSR.</p>"},{"location":"active-directory/group-policy/#dns-server-checks","title":"DNS Server Checks","text":"<p>Open DNS Manager and confirm:</p> <ul> <li>Forward lookup zone exists  </li> <li>SRV records created  </li> <li>Global Catalog records present  </li> <li>Host (A) records for DC created  </li> </ul>"},{"location":"active-directory/group-policy/#essential-dns-records","title":"Essential DNS Records","text":"<p>Look inside: <code>Forward Lookup Zones \u2192 thelodgelab.local</code></p> <p>You should see:</p> <ul> <li>(A) DC-LAB \u2192 192.168.199.210  </li> <li>_ldap._tcp.dc._msdcs SRV record  </li> <li>_kerberos._tcp.dc._msdcs SRV record  </li> </ul> <p>These verify: - Kerberos is operational - LDAP is reachable - The DC is advertising correctly  </p>"},{"location":"active-directory/group-policy/#dns-testing","title":"DNS Testing","text":"<p>```cmd nslookup thelodgelab.local nslookup DC-LAB dcdiag /test:dns Next Steps</p> <p>Add additional DCs (optional)</p> <p>Configure conditional forwarders</p> <p>Harden DNS security with DNSSEC (optional)</p>"},{"location":"active-directory/group-policy/#5-active-directoryhealthmd","title":"\u2705 5. active-directory/health.md","text":"<p>```markdown</p>"},{"location":"active-directory/group-policy/#active-directory-health-tools","title":"Active Directory Health &amp; Tools","text":"<p>This document provides diagnostic tools and health checks for maintaining the LodgeLab domain.</p>"},{"location":"active-directory/group-policy/#core-health-tools","title":"Core Health Tools","text":""},{"location":"active-directory/group-policy/#1-dcdiag","title":"1. dcdiag","text":"<p>Run:</p> <p>```cmd dcdiag Validates:</p> <p>DNS</p> <p>SYSVOL</p> <p>Replication</p> <p>Services</p> <p>Authentication paths</p> <ol> <li>repadmin repadmin /replsummary repadmin /showrepl</li> </ol> <p>Useful once you add a second Domain Controller.</p> <ol> <li>SYSVOL Status net share</li> </ol> <p>Verify:</p> <p>SYSVOL NETLOGON</p> <p>Both must appear for a healthy DC.</p> <p>Common Warnings (Expected on New DCs)</p> <p>LDAP signing not set</p> <p>Channel binding token warning</p> <p>NTP source not authoritative</p> <p>DNS delegation warning</p> <p>Single-DC replication warnings</p> <p>These are normal for a lab and can be addressed later.</p> <p>When to Fix Warnings</p> <p>Fix warnings when:</p> <p>Deploying a second Domain Controller</p> <p>Preparing for production hybrid use</p> <p>Hardening security</p> <p>Implementing PKI</p> <p>Next Steps</p> <p>Add 2nd DC (optional)</p> <p>Build Certificate Authority</p> <p>Implement Group Policy Baselines</p>"},{"location":"active-directory/group-policy/#done-agent","title":"\ud83d\udfe6 Done, Agent.","text":"<p>Your Active Directory documentation folder is now fully scaffolded, clean, professional, and ready for deeper work as your lab grows.</p> <p>If you'd like, I can also generate:</p> <ul> <li>A full AD topology diagram (PNG/SVG)</li> <li>A printable PDF version of your DC Build Guide</li> <li>A Case File\u2013style redacted FBI-themed AD overview (Twin Peaks aesthetic)</li> </ul> <p>Just say the word.</p>"},{"location":"active-directory/lab-dc-build/","title":"Building the LodgeLab Domain Controller","text":"<p>Windows Server 2019 on Proxmox (Q35 + VirtIO + AD DS)</p>"},{"location":"active-directory/lab-dc-build/#overview","title":"Overview","text":"<p>This guide documents the full process of building a Windows Server 2019 Domain Controller inside Proxmox for the LodgeLab environment. It covers:</p> <ul> <li>Preparing Proxmox storage for ISOs  </li> <li>Fixing QEMU/boot errors  </li> <li>Configuring VM hardware for Windows  </li> <li>Installing Windows Server using VirtIO drivers  </li> <li>Adding network drivers  </li> <li>Installing Active Directory Domain Services  </li> <li>Promoting the server to a Domain Controller  </li> <li>Validating with <code>dcdiag</code></li> </ul> <p>This guide reflects the exact real-time setup performed on theblacklodge node.</p>"},{"location":"active-directory/lab-dc-build/#1-preparing-iso-storage-on-proxmox","title":"1. Preparing ISO Storage on Proxmox","text":""},{"location":"active-directory/lab-dc-build/#11-confirm-correct-storage-path","title":"1.1 Confirm correct storage path","text":"<p>We discovered that the real ISO directory was:</p> <p>/mnt/mypool/isos/template/iso/</p> <p>This contained:</p> <ul> <li>Windows Server ISO  </li> <li>VirtIO driver ISO  </li> </ul>"},{"location":"active-directory/lab-dc-build/#12-create-or-edit-proxmox-storage-entry","title":"1.2 Create or edit Proxmox storage entry","text":"<p>Navigate:</p> <p>Datacenter \u2192 Storage \u2192 Add \u2192 Directory</p> <p>Or edit an existing one:</p> <p>ID: mypool-iso Directory: /mnt/mypool/isos/template/iso Content: ISO image Nodes: theblacklodge Enable: \u2611 Shared: \u2610</p> <p>After this, ISOs appeared properly in the VM creation menu.</p>"},{"location":"active-directory/lab-dc-build/#2-creating-the-windows-server-vm-in-proxmox","title":"2. Creating the Windows Server VM in Proxmox","text":""},{"location":"active-directory/lab-dc-build/#21-general","title":"2.1 General","text":"<ul> <li>Name: <code>DC-LAB</code></li> <li>VMID: 700 (your choice)</li> </ul>"},{"location":"active-directory/lab-dc-build/#22-os","title":"2.2 OS","text":"<ul> <li>Use Windows Server 2019 ISO </li> <li>use TPM for this lab.</li> </ul>"},{"location":"active-directory/lab-dc-build/#23-system","title":"2.3 System","text":"<ul> <li>BIOS: UEFI  </li> <li>Machine: <code>q35</code> </li> <li>Display: Default  </li> <li>Add SCSI Controller: VirtIO SCSI (single)</li> </ul>"},{"location":"active-directory/lab-dc-build/#24-hard-disk","title":"2.4 Hard Disk","text":"<ul> <li>Bus: SCSI  </li> <li>Size: 80GB  </li> <li>Storage: mypool  </li> <li>Cache: none  </li> <li>IO thread: enabled  </li> <li>Format: qcow2</li> </ul>"},{"location":"active-directory/lab-dc-build/#25-cpu-memory","title":"2.5 CPU &amp; Memory","text":"<ul> <li>2 cores (host type) </li> <li>8GB RAM</li> </ul>"},{"location":"active-directory/lab-dc-build/#26-network","title":"2.6 Network","text":"<ul> <li>Model: Intel E1000  </li> <li>Bridge: <code>vmbr0</code> </li> <li>(We later confirmed <code>vmbr1</code> did not exist)</li> </ul>"},{"location":"active-directory/lab-dc-build/#27-boot-order","title":"2.7 Boot order","text":"<p>Make sure:</p> <ol> <li>CD-ROM first  </li> <li>Disk second  </li> </ol>"},{"location":"active-directory/lab-dc-build/#3-fixing-the-qemu-start-error","title":"3. Fixing the QEMU Start Error","text":"<p>You encountered:</p> <p>start failed: QEMU exited with code 1 bridge 'vmbr1' does not exist</p> <p>Solution:</p> <p>Edit <code>/etc/pve/qemu-server/700.conf</code> Changed:</p> <pre><code>net0: e1000=XX:XX:XX,bridge=vmbr1\n</code></pre> <p>to:</p> <pre><code>bridge=vmbr0\n</code></pre> <p>Then the VM booted successfully.</p>"},{"location":"active-directory/lab-dc-build/#4-installing-windows-server","title":"4. Installing Windows Server","text":""},{"location":"active-directory/lab-dc-build/#41-windows-installer-did-not-detect-disk","title":"4.1 Windows Installer did not detect disk","text":"<p>Because your VM uses VirtIO SCSI, the Windows installer needed additional drivers.</p>"},{"location":"active-directory/lab-dc-build/#42-load-scsi-driver","title":"4.2 Load SCSI Driver","text":"<p>Click:</p> <p>Load Driver \u2192 Browse \u2192 viostor \u2192 2k19 \u2192 amd64</p> <p>Select:</p> <ul> <li>Red Hat VirtIO SCSI Controller</li> </ul> <p>After loading, the 80GB disk appeared.</p>"},{"location":"active-directory/lab-dc-build/#43-fixing-windows-install-error","title":"4.3 Fixing Windows install error","text":"<p>You briefly saw:</p> <pre><code>Windows cannot be installed to this disk. This computer's hardware may not support booting to this disk.\n</code></pre> <p>Solution:</p> <ul> <li>Delete the partition  </li> <li>Recreate New partition  </li> <li>Format  </li> <li>Proceed  </li> </ul> <p>Windows installed successfully.</p>"},{"location":"active-directory/lab-dc-build/#5-post-install-fix-network-driver-missing","title":"5. Post-Install Fix: Network Driver Missing","text":"<p>After login, Windows showed No Internet.</p>"},{"location":"active-directory/lab-dc-build/#fix","title":"Fix:","text":"<p>Mount the VirtIO ISO again, then:</p> <p>Device Manager \u2192 Other Devices \u2192 PCI Device \u2192 Update Driver</p> <p>Browse to:</p> <pre><code>NetKVM \u2192 2k19 \u2192 amd64\n</code></pre> <p>Once loaded, network became active and DHCP supplied:</p> <pre><code>192.168.199.210\n</code></pre> <p>Gateway/DNS verified.</p>"},{"location":"active-directory/lab-dc-build/#6-installing-active-directory-domain-services-ad-ds","title":"6. Installing Active Directory Domain Services (AD DS)","text":""},{"location":"active-directory/lab-dc-build/#61-add-the-role","title":"6.1 Add the Role","text":"<p>Open Server Manager \u2192 Add Roles and Features</p> <p>Select:</p> <ul> <li>Active Directory Domain Services</li> <li>DNS installs automatically</li> </ul>"},{"location":"active-directory/lab-dc-build/#62-promote-to-domain-controller","title":"6.2 Promote to Domain Controller","text":"<p>After installation, click:</p> <p>Promote this server to a domain controller</p> <p>Choose:</p> <ul> <li>Add a new forest</li> <li>Domain name: <code>thelorgelab.local</code> (example used: thelodgelab.local)</li> </ul> <p>No DNS delegation is needed \u2014 ignore the warning.</p>"},{"location":"active-directory/lab-dc-build/#63-paths","title":"6.3 Paths","text":"<p>Use defaults:</p> <pre><code>C:\\Windows\\NTDS\nC:\\Windows\\SYSVOL\n</code></pre>"},{"location":"active-directory/lab-dc-build/#64-reboot","title":"6.4 Reboot","text":"<p>Upon reboot, Windows logs in under:</p> <pre><code>thelorgelab\\Administrator\n</code></pre>"},{"location":"active-directory/lab-dc-build/#7-validating-domain-controller-health","title":"7. Validating Domain Controller Health","text":"<p>Run:</p> <pre><code>dcdiag\n</code></pre> <p>We validated:</p> <ul> <li>SYSVOL shared</li> <li>DNS service registered</li> <li>KCC functioning</li> <li>Replication tests (only warnings\u2014normal for brand-new single DC)</li> <li>Role holders (RID, PDC, Infrastructure) assigned</li> </ul> <p>All critical tests passed.</p>"},{"location":"active-directory/lab-dc-build/#8-current-status","title":"8. Current Status","text":"<p>Your LodgeLab Domain Controller is now: <pre><code>\u2714 Fully installed\n\u2714 Running AD DS\n\u2714 Running DNS\n\u2714 SYSVOL healthy\n\u2714 Network online\n\u2714 Ready for clients, OUs, GPOs, CA, NPS, file servers, hybrid Azure join, etc.\n</code></pre></p>"},{"location":"active-directory/lab-dc-build/#9-next-steps-phase-2-options","title":"9. Next Steps (Phase 2 Options)","text":"<p>Choose your direction when ready:</p>"},{"location":"active-directory/lab-dc-build/#option-a-build-organizational-units-users","title":"Option A \u2014 Build Organizational Units &amp; Users","text":"<ul> <li>Create the Lodge OU structure</li> <li>Create IT accounts, service accounts</li> <li>Admin tiering (Tier 0/1/2)</li> </ul>"},{"location":"active-directory/lab-dc-build/#option-b-join-a-windows-client-to-the-domain","title":"Option B \u2014 Join a Windows Client to the Domain","text":"<ul> <li>Build a Windows 10/11 VM</li> <li>Join <code>thelorgelab.local</code></li> </ul>"},{"location":"active-directory/lab-dc-build/#option-c-install-certificate-authority-ad-cs","title":"Option C \u2014 Install Certificate Authority (AD CS)","text":"<ul> <li>Needed for RDP, LDAPS, Wi-Fi authentication, Intune hybrid cert, etc.</li> </ul>"},{"location":"active-directory/lab-dc-build/#option-d-build-group-policies","title":"Option D \u2014 Build Group Policies","text":"<ul> <li>Security baselines</li> <li>Drive maps</li> <li>Lockdowns</li> <li>Login scripts</li> </ul>"},{"location":"active-directory/lab-dc-build/#option-e-hybrid-azure-ad","title":"Option E \u2014 Hybrid Azure AD","text":"<ul> <li>AAD Connect</li> <li>Sync OUs</li> <li>Password hash or seamless SSO</li> <li>Prep for Azure AD-only migration</li> </ul>"},{"location":"active-directory/lab-dc-build/#change-log","title":"Change Log","text":"Date Change 2025-12-07 Initial documentation created based on live build session"},{"location":"active-directory/ou-structure/","title":"LodgeLab OU &amp; User Structure","text":"<p>This document defines the recommended Organizational Unit (OU) hierarchy and user account structure for the LodgeLab domain.</p>"},{"location":"active-directory/ou-structure/#ou-design-philosophy","title":"OU Design Philosophy","text":"<p>The OU layout follows three core principles:</p> <ol> <li>Separation of roles \u2014 servers, workstations, and user objects stay isolated  </li> <li>Delegation clarity \u2014 permissions are applied to specific OUs  </li> <li>GPO targeting \u2014 policies are applied cleanly without overreach  </li> </ol>"},{"location":"active-directory/ou-structure/#recommended-ou-layout","title":"Recommended OU Layout","text":"<pre><code>thelorgelab.local\n\u2502\n\u251c\u2500\u2500 _Admin\n\u2502 \u251c\u2500\u2500 Admin Accounts\n\u2502 \u2514\u2500\u2500 Service Accounts\n\u2502\n\u251c\u2500\u2500 Users\n\u2502 \u251c\u2500\u2500 IT\n\u2502 \u251c\u2500\u2500 Staff\n\u2502 \u2514\u2500\u2500 Testing\n\u2502\n\u251c\u2500\u2500 Computers\n\u2502 \u251c\u2500\u2500 Workstations\n\u2502 \u2514\u2500\u2500 Laptops\n\u2502\n\u251c\u2500\u2500 Servers\n\u2502 \u251c\u2500\u2500 Core Infrastructure\n\u2502 \u2514\u2500\u2500 Application Servers\n\u2502\n\u2514\u2500\u2500 Groups\n\u251c\u2500\u2500 Security Groups\n\u251c\u2500\u2500 Distribution Groups\n\u2514\u2500\u2500 Role-Based Access Groups\n</code></pre>"},{"location":"active-directory/ou-structure/#user-types","title":"User Types","text":"<ul> <li>Standard Users: Normal employees or household users  </li> <li>Privileged Admins: Domain Admins, Server Admins, helpdesk roles  </li> <li>Service Accounts: For applications, automation, and scheduled tasks  </li> <li>Test Users: For lab experiments and GPO validation  </li> </ul>"},{"location":"active-directory/ou-structure/#next-steps","title":"Next Steps","text":"<ul> <li>Create OUs in Active Directory Users and Computers </li> <li>Move existing computer objects into correct OUs  </li> <li>Apply GPOs to the correct OU targets  </li> </ul>"},{"location":"casefile/","title":"Case File Index","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"casefile/addendums/","title":"Addendums","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"casefile/briefings/","title":"Briefings","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"casefile/owl-sightings/","title":"Owl Sightings","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/","title":"Secure Cloudflare Tunnel Access to Code-Server (pacard-sawmill)","text":"<p>This document outlines the full configuration used to expose <code>code-server</code> on pacard-sawmill securely over the internet using Cloudflare Tunnel and Cloudflare Zero Trust. This ensures maximum security, no LAN exposure, and MFA-protected access from anywhere.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#overview-2","title":"Overview-2","text":"<p>This setup provides:</p> <ul> <li>Secure remote access to code-server</li> <li>Zero Trust identity protection (email + MFA)</li> <li>No VPN required</li> <li>No ports opened on your home network</li> <li>code-server bound only to localhost for safety</li> <li>Cloudflare Tunnel handles all public traffic</li> </ul> <p>Traffic flow:</p> <pre><code>You \u2192 Cloudflare Zero Trust \u2192 Cloudflare Tunnel \u2192 pacard-sawmill \u2192 code-server\n</code></pre> <p>LAN access is intentionally disabled.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#1-install-cloudflared-on-pacard-sawmill","title":"1. Install cloudflared on pacard-sawmill","text":"<pre><code>curl -fsSL https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb -o cloudflared.deb\nsudo dpkg -i cloudflared.deb\n</code></pre> <p>Authenticate the instance:</p> <pre><code>cloudflared tunnel login\n</code></pre> <p>Choose the domain osirispc.com.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#2-create-the-tunnel","title":"2. Create the Tunnel","text":"<pre><code>cloudflared tunnel create pacard\n</code></pre> <p>This creates credentials:</p> <pre><code>/home/osirisortiz/.cloudflared/&lt;UUID&gt;.json\n</code></pre>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#3-configure-the-tunnel","title":"3. Configure the Tunnel","text":"<p>Create and edit the config:</p> <pre><code>sudo mkdir -p /etc/cloudflared\nsudo nano /etc/cloudflared/config.yml\n</code></pre> <p>Paste:</p> <pre><code>tunnel: pacard\ncredentials-file: /home/osirisortiz/.cloudflared/&lt;UUID&gt;.json\n\ningress:\n  - hostname: code.osirispc.com\n    service: http://127.0.0.1:8080\n  - service: http_status:404\n</code></pre> <p>Replace <code>&lt;UUID&gt;</code> accordingly.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#4-create-dns-route","title":"4. Create DNS Route","text":"<pre><code>cloudflared tunnel route dns pacard code.osirispc.com\n</code></pre> <p>This creates the necessary CNAME record.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#5-install-and-start-the-tunnel-service","title":"5. Install and Start the Tunnel Service","text":"<pre><code>sudo cloudflared service install\nsudo systemctl enable cloudflared\nsudo systemctl start cloudflared\n</code></pre> <p>Verify:</p> <pre><code>systemctl status cloudflared\n</code></pre> <p>Should show: Active (running).</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#6-secure-with-cloudflare-zero-trust","title":"6. Secure with Cloudflare Zero Trust","text":"<p>Navigate to:</p> <pre><code>Cloudflare Dashboard \u2192 Zero Trust \u2192 Access \u2192 Applications \u2192 Add Application\n</code></pre> <p>Select Self-hosted.</p> <p>Configure:</p> <ul> <li>Domain: <code>code.osirispc.com</code></li> <li>Name: Code Server</li> </ul> <p>Access policy:</p> <pre><code>Action: Allow\nSelector: Emails\nValue: osirisortizpc@gmail.com\n</code></pre> <p>This requires identity login + MFA before accessing code-server.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#7-configure-code-server-to-bind-locally-only","title":"7. Configure code-server to Bind Locally Only","text":"<p>File:</p> <pre><code>~/.config/code-server/config.yaml\n</code></pre> <p>Recommended:</p> <pre><code>auth: none\nbind-addr: 127.0.0.1:8080\ncert: false\n</code></pre> <p>Restart code-server:</p> <pre><code>pkill -f code-server\ncode-server\n</code></pre>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#8-final-behavior-summary","title":"8. Final Behavior Summary","text":""},{"location":"cloudflare/cloudflare-tunnel-code-server/#external-access-works","title":"External access (WORKS):","text":"<pre><code>https://code.osirispc.com\n</code></pre>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#local-machine-access-works","title":"Local machine access (WORKS):","text":"<pre><code>http://127.0.0.1:8080\n</code></pre>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#lan-access-blocked-intentionally","title":"LAN access (BLOCKED intentionally):","text":"<pre><code>http://192.168.199.177:8080\n</code></pre> <p>This is the expected Zero Trust behavior.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#optional-enable-lan-access-later","title":"Optional: Enable LAN Access Later","text":"<p>Change:</p> <pre><code>bind-addr: 0.0.0.0:8080\n</code></pre> <p>Restart code-server.</p> <p>LAN devices will now reach the service.</p>"},{"location":"cloudflare/cloudflare-tunnel-code-server/#result","title":"Result","text":"<p>You now have:</p> <ul> <li>A secure, MFA-protected Cloudflare Tunnel</li> <li>A globally accessible development environment</li> <li>Zero exposure on your LAN</li> <li>A professional Zero Trust setup</li> <li>Fully isolated code-server endpoint</li> </ul> <p>This configuration is ideal for a hardened, flexible, secure homelab.</p>"},{"location":"cloudflare/cloudflare-zero-trust-setup/","title":"Cloudflare Zero Trust + Tunnel Setup Guide","text":"<p>A complete step-by-step reference for securing homelab services behind Cloudflare.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#overview","title":"\ud83c\udf10 Overview","text":"<p>This guide explains how to:</p> <ol> <li>Move DNS to Cloudflare  </li> <li>Set up Cloudflare Zero Trust  </li> <li>Create a Cloudflare Tunnel  </li> <li>Protect services with Access  </li> <li>Configure public hostnames  </li> <li>Test, verify, and troubleshoot  </li> </ol> <p>All commands and YAML snippets are in fenced code blocks with MkDocs copy-buttons.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#1-move-dns-to-cloudflare","title":"1\ufe0f\u20e3 Move DNS to Cloudflare","text":"","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-1-add-your-domain","title":"Step 1 \u2014 Add your domain","text":"<p>Cloudflare Dashboard \u2192 Add a site</p> <p>Enter your domain: <pre><code>osirispc.com\n</code></pre></p> <p>Select the free plan and continue.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-2-change-nameservers-at-your-registrar","title":"Step 2 \u2014 Change nameservers at your registrar","text":"<p>Cloudflare will assign two nameservers, for example:</p> <pre><code>bowen.ns.cloudflare.com\npaige.ns.cloudflare.com\n</code></pre> <p>At GoDaddy (or your registrar):</p> <ol> <li>Go to DNS \u2192 Nameservers \u2192 Change</li> <li>Enter Cloudflare nameservers</li> <li>Save</li> </ol>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-3-verify-nameserver-propagation","title":"Step 3 \u2014 Verify nameserver propagation","text":"<p>Visit: <pre><code>https://dnschecker.org/#NS/osirispc.com\n</code></pre></p> <p>Resolvers should show only Cloudflare nameservers.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#2-enable-cloudflare-zero-trust","title":"2\ufe0f\u20e3 Enable Cloudflare Zero Trust","text":"<p>Cloudflare Dashboard \u2192 Zero Trust</p> <p>Follow the initial setup prompts.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-1-configure-identity-providers","title":"Step 1 \u2014 Configure identity providers","text":"<p>Common choices:</p> <ul> <li>Google  </li> <li>GitHub  </li> <li>One-time PIN  </li> <li>Azure AD  </li> </ul> <p>Enable whichever your organization uses.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-2-set-an-organization-login-domain","title":"Step 2 \u2014 Set an organization login domain","text":"<p>Example: <pre><code>osirispc.cloudflareaccess.com\n</code></pre></p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#3-install-create-cloudflare-tunnel","title":"3\ufe0f\u20e3 Install &amp; Create Cloudflare Tunnel","text":"<p>SSH into the server hosting your internal services.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-1-install-cloudflared","title":"Step 1 \u2014 Install cloudflared","text":"","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#debianubuntu","title":"Debian/Ubuntu:","text":"<pre><code>curl -L https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb \\\n  -o cloudflared.deb\n\nsudo dpkg -i cloudflared.deb\n</code></pre>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-2-authenticate-cloudflared","title":"Step 2 \u2014 Authenticate cloudflared","text":"<pre><code>cloudflared tunnel login\n</code></pre> <p>Follow the browser link and authenticate.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-3-create-a-named-tunnel","title":"Step 3 \u2014 Create a named tunnel","text":"<pre><code>cloudflared tunnel create code-tunnel\n</code></pre> <p>A credentials file will be generated.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-4-configure-ingress-rules","title":"Step 4 \u2014 Configure ingress rules","text":"<p>Create: <pre><code>/etc/cloudflared/config.yml\n</code></pre></p> <p>Example: <pre><code>tunnel: code-tunnel\ncredentials-file: /etc/cloudflared/&lt;your-tunnel-uuid&gt;.json\n\ningress:\n  - hostname: code.osirispc.com\n    service: http://localhost:8002\n\n  - hostname: docs.osirispc.com\n    service: http://localhost:8282\n\n  - service: http_status:404\n</code></pre></p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#step-5-install-start-cloudflared-service","title":"Step 5 \u2014 Install + Start cloudflared service","text":"<pre><code>sudo cloudflared service install\nsudo systemctl enable cloudflared\nsudo systemctl start cloudflared\n</code></pre> <p>Tunnel is now active.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#4-add-public-hostnames-in-cloudflare-tunnel","title":"4\ufe0f\u20e3 Add Public Hostnames in Cloudflare Tunnel","text":"<p>Cloudflare Dashboard \u2192 Zero Trust \u2192 Networks \u2192 Tunnels \u2192 code-tunnel \u2192 Public Hostnames</p> <p>Click Add Public Hostname</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#example-docs-site","title":"Example \u2014 Docs site","text":"<p>Hostname: <pre><code>docs.osirispc.com\n</code></pre></p> <p>Service: <pre><code>http://192.168.199.177:8282\n</code></pre></p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#example-code-server","title":"Example \u2014 Code Server","text":"<p>Hostname: <pre><code>code.osirispc.com\n</code></pre></p> <p>Service: <pre><code>http://192.168.199.177:8002\n</code></pre></p> <p>Cloudflare automatically creates DNS CNAMEs like: <pre><code>docs.osirispc.com \u2192 &lt;tunnel-id&gt;.cfargotunnel.com\n</code></pre></p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#5-protect-applications-with-cloudflare-access","title":"5\ufe0f\u20e3 Protect Applications With Cloudflare Access","text":"<p>Cloudflare Dashboard \u2192 Zero Trust \u2192 Access \u2192 Applications \u2192 Add Application</p> <p>Select Self-hosted.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#example-protect-docsosirispccom","title":"Example \u2014 Protect docs.osirispc.com","text":"<p>Application Name: <pre><code>docs\n</code></pre></p> <p>Application Domain: <pre><code>docs.osirispc.com/*\n</code></pre></p> <p>Policy: Allow \u2192 Email: <pre><code>osirisortizpc@gmail.com\n</code></pre></p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#example-protect-codeosirispccom","title":"Example \u2014 Protect code.osirispc.com","text":"<p>Application Domain: <pre><code>code.osirispc.com/*\n</code></pre></p> <p>Policy: Allow \u2192 Email: <pre><code>osirisortizpc@gmail.com\n</code></pre></p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#6-testing-cloudflare-zero-trust","title":"6\ufe0f\u20e3 Testing Cloudflare Zero Trust","text":"","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#test-1-visit-your-protected-service","title":"Test 1 \u2014 Visit your protected service","text":"<pre><code>https://docs.osirispc.com\n</code></pre> <p>You should see the Cloudflare Access login.</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#test-2-identity-diagnostic","title":"Test 2 \u2014 Identity diagnostic","text":"<p>Visit: <pre><code>https://whoami.cloudflareaccess.com\n</code></pre></p> <p>Expected output includes: <pre><code>isAccessGranted: true\nemail: your_email\n</code></pre></p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#test-3-check-dns-resolution","title":"Test 3 \u2014 Check DNS resolution","text":"<p>Run: <pre><code>dig docs.osirispc.com\n</code></pre></p> <p>You should see: <pre><code>docs.osirispc.com.  CNAME  &lt;tunnel-id&gt;.cfargotunnel.com.\n</code></pre></p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#7-troubleshooting","title":"7\ufe0f\u20e3 Troubleshooting","text":"","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#if-you-get-the-welcome-no-apps-assigned-screen","title":"\u274c If you get the \u201cWelcome \u2014 no apps assigned\u201d screen:","text":"<p>Cloudflare is authenticating you but can\u2019t match the hostname to an Access App.</p> <p>Fix:</p> <ol> <li>Application Domain must be:    <pre><code>docs.osirispc.com/*\n</code></pre></li> <li>Public Hostname must exist in the tunnel  </li> <li>DNS CNAME must exist  </li> <li>Policy must include your email  </li> </ol>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#changelog","title":"\ud83e\uddfe Changelog","text":"Date Author Description 2025-11-29 Osiris Ortiz Initial creation of Cloudflare + Zero Trust guide 2025-11-29 Agent Cooper Added changelog, metadata, and nav integration notes","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"cloudflare/cloudflare-zero-trust-setup/#document-status","title":"\ud83d\uddc2\ufe0f Document Status","text":"<p>Last reviewed: 2025-11-29 Version: 1.0</p>","tags":["cloudflare","zero-trust","tunnels","homelab-security"]},{"location":"extra/briefings/","title":"Agent Briefings","text":"<p>High-level summaries for future you, future bosses, and any other cleared personnel.</p>"},{"location":"extra/owls/","title":"Owl Sightings","text":"<p>Document all unusual network or system behavior here.</p>"},{"location":"extra/red-room/","title":"Red Room Notes","text":"<p>Initial log. Further entries to be filed by Agent Ortiz.</p>"},{"location":"guides/cleanup-downloads/","title":"\ud83e\uddf9 Automated Downloads Cleanup (RARR Stack)","text":"<p>Your automated cleanup system on thewhitelodge removes old, completed downloads from:</p> <p>/mnt/data/downloads</p> <p>while preserving:</p> <ul> <li>\ud83d\udfea <code>/mnt/data/downloads/incomplete/</code></li> <li>\ud83c\udd95 Newly imported items</li> <li>\ud83d\uddd3\ufe0f Anything younger than 3 days (configurable)</li> </ul> <p>This prevents clutter, saves storage, and keeps Radarr/Sonarr working smoothly.</p>"},{"location":"guides/cleanup-downloads/#directory-structure","title":"\ud83d\udcc1 Directory Structure","text":"<p>Your downloads share is laid out like:</p> <p>/mnt/data/downloads \u251c\u2500\u2500 incomplete/      # DO NOT DELETE \u2014 active downloads \u251c\u2500\u2500 Movie1/ \u251c\u2500\u2500 Movie2/ \u251c\u2500\u2500 TV Show/ \u2514\u2500\u2500 Other items...</p> <p>Everything except <code>incomplete/</code> is eligible for cleanup once it's older than X days.</p>"},{"location":"guides/cleanup-downloads/#cleanup-script","title":"\ud83e\uddfd Cleanup Script","text":""},{"location":"guides/cleanup-downloads/#script-location","title":"\ud83d\udccd Script Location","text":"<p>/usr/local/bin/cleanup-downloads.sh</p>"},{"location":"guides/cleanup-downloads/#script-contents","title":"\ud83d\udcdd Script Contents","text":"<pre><code>#!/bin/bash\n# cleanup-downloads.sh\n# Automatically clean up completed RARR downloads older than X days.\n\n\nDOWNLOADS_DIR=\"/mnt/data/downloads\"\nINCOMPLETE_NAME=\"incomplete\"\nLOGFILE=\"/var/log/cleanup-downloads.log\"\nDAYS_OLD=3   # Delete items older than X days\n\necho \"===== Cleanup run: $(date) =====\" &gt;&gt; \"$LOGFILE\"\n\nfind \"$DOWNLOADS_DIR\" \\\n  -mindepth 1 \\\n  -maxdepth 1 \\\n  ! -name \"$INCOMPLETE_NAME\" \\\n  -type d \\\n  -mtime +\"$DAYS_OLD\" \\\n  -print -exec rm -rf {} \\; &gt;&gt; \"$LOGFILE\" 2&gt;&amp;1\n\necho \"Cleanup complete.\" &gt;&gt; \"$LOGFILE\"\n</code></pre> <p>\ud83d\udd10 Set Permissions</p> <pre><code>sudo chmod +x /usr/local/bin/cleanup-downloads.sh\n</code></pre> <p>\u23f1\ufe0f Enable Nightly Cron Job (2:00 AM)</p> <p>Open the root crontab:</p> <p><pre><code>sudo crontab -e\n</code></pre> Add:</p> <pre><code>0 2 * * * /usr/local/bin/cleanup-downloads.sh\n</code></pre> <p>\ud83e\uddea Testing &amp; Verification</p> <p>\u25b6\ufe0f Run Manually</p> <pre><code>sudo /usr/local/bin/cleanup-downloads.sh\n</code></pre> <p>\ud83e\uddd0 Dry Run (See What WOULD Be Deleted) <pre><code>sudo find /mnt/data/downloads \\\n  -mindepth 1 \\\n  -maxdepth 1 \\\n  ! -name incomplete \\\n  -type d \\\n  -mtime +3 \\\n  -print\n</code></pre> \ud83d\udcc4 Check Logs <pre><code>/var/log/cleanup-downloads.log\n</code></pre> Tail: <pre><code>tail -n 50 /var/log/cleanup-downloads.log\n</code></pre></p> <p>\ud83e\udeb5 Change Log</p> <p>2025-11-30 \u2014 Initial version</p> <p>Added X-day retention</p> <p>Cleanup exclusions</p> <p>Crontab automation</p> <p>Logging support</p>"},{"location":"guides/docker/","title":"docker","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"guides/mkdocs-dev-setup/","title":"\ud83c\udf32 Homelab Docs \u2013 MkDocs Dev Environment Startup (SOP)","text":"<p>Purpose: This procedure starts the MkDocs development server for the Homelab Docs project with full live-reload support and reliable file watching while using code-server.</p> <p>Use this every time you want to edit or work on your documentation.</p>"},{"location":"guides/mkdocs-dev-setup/#step-1-open-the-project-directory","title":"\u2705 Step 1 \u2014 Open the Project Directory","text":"<pre><code>cd ~/projects/homelab-docs\n</code></pre>"},{"location":"guides/mkdocs-dev-setup/#what-this-does","title":"What this does:","text":"<p>Moves your terminal into the Homelab Docs project folder so all commands run in the correct location.</p>"},{"location":"guides/mkdocs-dev-setup/#step-2-activate-the-python-virtual-environment","title":"\u2705 Step 2 \u2014 Activate the Python Virtual Environment","text":"<pre><code>source .venv/bin/activate\n</code></pre> <p>You should now see:</p> <pre><code>(.venv)\n</code></pre>"},{"location":"guides/mkdocs-dev-setup/#what-this-does_1","title":"What this does:","text":"<p>This activates your isolated Python environment, ensuring:</p> <ul> <li>MkDocs runs with the correct dependencies</li> <li>System Python is never altered</li> <li>Package conflicts are avoided</li> </ul> <p>If <code>(.venv)</code> does not appear, stop here and do not continue.</p>"},{"location":"guides/mkdocs-dev-setup/#step-3-enable-reliable-file-watching-required-for-code-server","title":"\u2705 Step 3 \u2014 Enable Reliable File Watching (Required for code-server)","text":"<pre><code>export MKDOCS_WATCH=1\n</code></pre>"},{"location":"guides/mkdocs-dev-setup/#what-this-does_2","title":"What this does:","text":"<p>Forces MkDocs to use polling mode instead of standard file watchers.</p> <p>This is required because:</p> <ul> <li>code-server writes files in a way that breaks default file watching</li> <li>Without this, changes may not reload</li> <li>With this, every edit is detected reliably</li> </ul>"},{"location":"guides/mkdocs-dev-setup/#step-4-start-the-mkdocs-development-server","title":"\u2705 Step 4 \u2014 Start the MkDocs Development Server","text":"<pre><code>mkdocs serve -a 0.0.0.0:8002\n</code></pre>"},{"location":"guides/mkdocs-dev-setup/#what-this-does_3","title":"What this does:","text":"<ul> <li>Starts the live development web server</li> <li>Enables automatic page reload on file changes</li> <li>Binds the service to all network interfaces</li> <li>Makes the site accessible from other devices on the network</li> </ul>"},{"location":"guides/mkdocs-dev-setup/#access-the-documentation-site","title":"\ud83c\udf10 Access the Documentation Site","text":"<p>Open your browser to:</p> <pre><code>http://192.168.199.177:8002/\n</code></pre> <p>Your site is now live with instant updates.</p>"},{"location":"guides/mkdocs-dev-setup/#stopping-the-server","title":"\u274c Stopping the Server","text":"<p>When you're finished working:</p> <pre><code>CTRL + C\n</code></pre>"},{"location":"guides/mkdocs-dev-setup/#what-this-does_4","title":"What this does:","text":"<ul> <li>Gracefully stops the MkDocs server</li> <li>Frees the network port</li> <li>Safely exits the development process</li> </ul>"},{"location":"guides/mkdocs-dev-setup/#required-project-setting-one-time-setup","title":"\u2699\ufe0f Required Project Setting (One-Time Setup)","text":"<p>Your <code>mkdocs.yml</code> must contain this entry:</p> <pre><code>watch:\n  - docs\n</code></pre>"},{"location":"guides/mkdocs-dev-setup/#what-this-does_5","title":"What this does:","text":"<p>Ensures MkDocs always monitors the correct documentation directory for changes.</p>"},{"location":"guides/mkdocs-dev-setup/#summary-the-full-startup-flow","title":"\u2705 Summary \u2014 The Full Startup Flow","text":"<pre><code>cd ~/projects/homelab-docs\nsource .venv/bin/activate\nexport MKDOCS_WATCH=1\nmkdocs serve -a 0.0.0.0:8002\n</code></pre>"},{"location":"guides/mkdocs-dev-setup/#final-note","title":"Final Note","text":"<p>This workflow guarantees:</p> <ul> <li>Reliable live reload</li> <li>No missed file changes</li> <li>Clean Python isolation</li> <li>Network-accessible dev site</li> <li>Zero guesswork</li> </ul> <p>When this SOP is followed, the system behaves\u2026 predictably.</p> <p>And that\u2019s exactly how we like it.</p> <p>```</p>"},{"location":"guides/mkdocs-docker-instant-reload/","title":"MkDocs Instant Reload","text":"<p>Alright. Let\u2019s write it into your case file so Future-Osiris can pull it off the shelf and repeat it without a s\u00e9ance.</p> <p>Below is a ready-to-paste Markdown page plus the exact steps to place it and wire it into the sidebar.</p>"},{"location":"guides/mkdocs-docker-instant-reload/#step-1-create-the-new-doc-page","title":"Step 1) Create the new doc page","text":"<p>Machine: <code>pacard-sawmill</code> Path: <code>/home/osirisortiz/projects/homelab-docs/docs/infra/mkdocs-docker-instant-reload.md</code></p> <p>Run:</p> <pre><code>nano /home/osirisortiz/projects/homelab-docs/docs/infra/mkdocs-docker-instant-reload.md\n</code></pre> <p>Paste this whole page:</p> <pre><code># MkDocs in Docker: Instant Reload Fix (Polling Watcher)\n\nWhen MkDocs runs inside a Docker container, it may not detect file changes from the host.\nSymptoms look like this:\n\n- You edit a `.md` file on the host\n- Browser shows old content\n- **Only updates after you restart the container**\n\nThis happens because Docker bind mounts sometimes **do not forward file-change events**\n(inotify/watchdog) into the container. So MkDocs never gets the \u201csomething changed\u201d signal.\n\nThe cure: **force polling** so MkDocs checks the filesystem on an interval.\n\n---\n\n## \u2705 The Golden Fix\n\nAdd polling env vars and an explicit serve command to your Docker Compose service.\n\n### Example `docker-compose.yml`\n\n```yml\nservices:\n  mkdocs:\n    image: squidfunk/mkdocs-material:9.5.31\n    container_name: mkdocs-dev\n    ports:\n      - \"8181:8000\"\n    volumes:\n      - /FULL/PATH/TO/YOUR/REPO:/docs\n    environment:\n      - WATCHFILES_FORCE_POLLING=true\n      - WATCHFILES_POLL_DELAY_MS=300\n    command: serve -a 0.0.0.0:8000 --livereload -w docs -w mkdocs.yml\n    restart: unless-stopped\n</code></pre>"},{"location":"guides/mkdocs-docker-instant-reload/#what-each-line-is-doing","title":"What each line is doing","text":"<ul> <li> <p><code>WATCHFILES_FORCE_POLLING=true</code>   Forces MkDocs\u2019 watcher to poll for changes instead of waiting for inotify events.</p> </li> <li> <p><code>WATCHFILES_POLL_DELAY_MS=300</code>   How often it checks for changes (300ms feels instant without burning CPU).</p> </li> <li> <p><code>command: serve ... --livereload -w docs -w mkdocs.yml</code>   Makes the container run the dev server with:</p> </li> <li> <p>livereload enabled</p> </li> <li> <p>explicit watch targets so it always rebuilds on edits.</p> </li> <li> <p><code>volumes: /repo:/docs</code>   Always mount the whole repo folder, not individual files.   Single-file mounts can \u201cfreeze\u201d because editors swap temp files/inodes.</p> </li> </ul>"},{"location":"guides/mkdocs-docker-instant-reload/#how-to-verify-its-working","title":"How to Verify It's Working","text":"<ol> <li>Start the container:</li> </ol> <pre><code>docker compose up -d --force-recreate\ndocker logs -f mkdocs-dev\n</code></pre> <ol> <li>You should see:</li> </ol> <pre><code>Watching paths for changes: 'docs', 'mkdocs.yml'\nServing on http://0.0.0.0:8000/\n</code></pre> <ol> <li> <p>Edit any markdown file under <code>docs/</code> and save.</p> </li> <li> <p>Logs should immediately show:</p> </li> </ol> <pre><code>Building documentation...\nDocumentation built in ...\n</code></pre> <p>If you see that without restarting the container, you're back on the good road.</p>"},{"location":"guides/mkdocs-docker-instant-reload/#notes","title":"Notes","text":"<ul> <li> <p>MkDocs will warn about <code>0.0.0.0</code> being \u201cproduction-like.\u201d   In Docker that\u2019s normal \u2014 it allows port mapping to your host.</p> </li> <li> <p>If your repo lives on a network share (NFS/CIFS),   increase polling delay (ex: 500\u20131000ms).</p> </li> </ul>"},{"location":"guides/mkdocs-docker-instant-reload/#quick-reference-snippet","title":"Quick Reference Snippet","text":"<pre><code>environment:\n  - WATCHFILES_FORCE_POLLING=true\n  - WATCHFILES_POLL_DELAY_MS=300\ncommand: serve -a 0.0.0.0:8000 --livereload -w docs -w mkdocs.yml\nvolumes:\n  - /FULL/PATH/TO/REPO:/docs\n</code></pre> <p>That\u2019s the whole spell.</p> <pre><code>Save and exit:\n- **Ctrl+O**, Enter\n- **Ctrl+X**\n\n---\n\n## Step 2) Wire it into the sidebar (mkdocs.yml)\n\n**File:**  \n`/home/osirisortiz/projects/homelab-docs/mkdocs.yml`\n\nOpen it:\n\n```bash\nnano /home/osirisortiz/projects/homelab-docs/mkdocs.yml\n</code></pre> <p>Find your <code>nav:</code> section. Add this entry wherever your Infra/docker docs live.</p> <p>Example:</p> <pre><code>nav:\n  - Home: index.md\n  - Infra:\n      - Overview: infra/index.md\n      - MkDocs Docker Instant Reload Fix: infra/mkdocs-docker-instant-reload.md\n</code></pre> <p>If you already have an Infra section with other pages, just drop the new line under it.</p> <p>Save + exit.</p>"},{"location":"guides/mkdocs-docker-instant-reload/#step-3-confirm-it-shows-up","title":"Step 3) Confirm it shows up","text":"<p>If your homelab docs dev container is running on port 8002, refresh:</p> <p><code>http://192.168.199.177:8002/</code></p> <p>You should see a new sidebar item: Infra \u2192 MkDocs Docker Instant Reload Fix</p> <p>If you paste your current <code>nav:</code> block here, I\u2019ll tell you the exact spot to insert the line so it matches your structure perfectly.</p>"},{"location":"guides/proxmox-tips/","title":"proxmox tips","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"guides/vm-setup/","title":"vm setup","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"infra/","title":"Infrastructure","text":"<p>Welcome to the Infrastructure files \u2014 the wiring behind the curtains.</p> <p>Here you\u2019ll find the baseline maps, DNS field notes, storage strategy, and replication methods that keep the Lodges standing.</p>"},{"location":"infra/#sections","title":"Sections","text":"<ul> <li>Overview</li> <li>Network Diagram</li> <li>Network Baseline</li> <li>DNS (Pi-hole + Unbound)</li> <li>Storage &amp; Backups</li> <li>ZFS &amp; VM Mirrors</li> </ul>"},{"location":"infra/dns-pihole-unbound/","title":"\ud83d\udcd8 Pi-hole + Unbound Deployment (Host Networking Mode)","text":"<p>A Lodge Infrastructure Case File</p>"},{"location":"infra/dns-pihole-unbound/#overview","title":"\ud83d\udcdd Overview","text":"<p>This document describes the complete setup of Pi-hole with Unbound running on an Ubuntu 24.04 virtual machine using Docker. This configuration gives you:</p> <ul> <li>Network-wide ad blocking</li> <li>Full recursive DNS (no 3rd-party DNS)</li> <li>Local hostname resolution</li> <li>High privacy</li> <li>Fast DNS caching</li> <li>Simple failover options</li> </ul> <p>Pi-hole operates in host networking mode for maximum reliability, while Unbound runs in a standard Docker bridge network (exposing port 5335).</p>"},{"location":"infra/dns-pihole-unbound/#architecture","title":"\ud83c\udfdb\ufe0f Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Lodge Network (LAN)                    \u2502\n\u2502    Devices \u2192 192.168.199.53 (Pi-hole DNS Server)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Pi-hole (Docker, host networking)             \u2502\n\u2502  - Listens directly on host ports: 53, 80, 443, 123    \u2502\n\u2502  - Forwards DNS to Unbound at 127.0.0.1#5335           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Unbound (Docker bridge)                   \u2502\n\u2502        Listens on 0.0.0.0:5335 (TCP+UDP)               \u2502\n\u2502   Recursively queries the DNS root servers             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"infra/dns-pihole-unbound/#vm-requirements","title":"\ud83d\udda5\ufe0f VM Requirements","text":"<ul> <li>Ubuntu Server 24.04 LTS</li> <li>Static IP:   192.168.199.53</li> <li>1 CPU / 1\u20132GB RAM minimum</li> <li>Docker installed</li> <li>UFW disabled or configured to allow DNS (port 53)</li> </ul>"},{"location":"infra/dns-pihole-unbound/#install-docker-docker-compose","title":"\ud83e\uddf0 Install Docker &amp; Docker Compose","text":"<pre><code>sudo apt update\nsudo apt install docker.io docker-compose -y\nsudo systemctl enable --now docker\n</code></pre>"},{"location":"infra/dns-pihole-unbound/#directory-structure","title":"\ud83d\udce6 Directory Structure","text":"<p>Create the directory:</p> <pre><code>mkdir -p /opt/pihole\ncd /opt/pihole\n</code></pre>"},{"location":"infra/dns-pihole-unbound/#docker-compose-file","title":"\ud83e\uddfe Docker Compose File","text":"<p>Create:</p> <pre><code>sudo nano /opt/pihole/docker-compose.yml\n</code></pre> <p>Paste:</p> <pre><code>version: \"3.3\"\n\nservices:\n  pihole:\n    container_name: pihole\n    image: pihole/pihole:latest\n    network_mode: host\n    environment:\n      TZ: \"America/New_York\"\n      WEBPASSWORD: \"admin\"\n      DNSMASQ_LISTENING: all\n    volumes:\n      - ./etc-pihole:/etc/pihole\n      - ./etc-dnsmasq.d:/etc/dnsmasq.d\n    restart: unless-stopped\n\n  unbound:\n    container_name: unbound\n    image: mvance/unbound:latest\n    ports:\n      - \"5335:53/tcp\"\n      - \"5335:53/udp\"\n    restart: unless-stopped\n</code></pre>"},{"location":"infra/dns-pihole-unbound/#deployment","title":"\u25b6\ufe0f Deployment","text":"<p>Run:</p> <pre><code>cd /opt/pihole\nsudo docker-compose down\nsudo docker-compose up -d --force-recreate\n</code></pre> <p>Verify the containers:</p> <pre><code>docker ps\n</code></pre> <p>You should see:</p> <ul> <li>Pi-hole running and healthy</li> <li>Unbound running on port 5335</li> </ul>"},{"location":"infra/dns-pihole-unbound/#testing-dns","title":"\ud83e\uddea Testing DNS","text":"<p>Test recursion and forwarding:</p> <pre><code>dig @192.168.199.53 google.com\ndig @192.168.199.53 cloudflare.com\ndig @192.168.199.53 nasa.gov\n</code></pre> <p>Expected output:</p> <ul> <li><code>status: NOERROR</code></li> <li><code>ANSWER SECTION</code></li> <li>Times varying from 1ms (cached) to 100+ms (recursive)</li> </ul>"},{"location":"infra/dns-pihole-unbound/#pi-hole-configuration","title":"\u2699\ufe0f Pi-hole Configuration","text":"<p>Go to:</p> <pre><code>http://192.168.199.53/admin\n</code></pre>"},{"location":"infra/dns-pihole-unbound/#upstream-dns","title":"Upstream DNS","text":"<p>Disable all upstream DNS providers.</p> <p>Add only:</p> <pre><code>127.0.0.1#5335\n</code></pre>"},{"location":"infra/dns-pihole-unbound/#advanced-dns-settings","title":"Advanced DNS settings","text":"<p>Enable:</p> <ul> <li>\u2714 Listen on all interfaces</li> <li>\u2714 Never forward non-FQDNs</li> <li>\u2714 Never forward reverse lookups for private ranges</li> </ul>"},{"location":"infra/dns-pihole-unbound/#optional-conditional-forwarding","title":"\ud83c\udfe0 Optional: Conditional Forwarding","text":"<p>To resolve local device names:</p> <p>Settings \u2192 DNS \u2192 Conditional Forwarding</p> <ul> <li>Local network: <code>192.168.199.0/24</code></li> <li>Router IP: <code>192.168.199.1</code></li> <li>Local domain name: <code>lan</code></li> </ul>"},{"location":"infra/dns-pihole-unbound/#router-configuration","title":"\ud83d\udce1 Router Configuration","text":"<p>Point LAN DNS to:</p> <pre><code>192.168.199.53\n</code></pre> <p>All clients will now use Pi-hole \u2192 Unbound \u2192 Root DNS servers.</p>"},{"location":"infra/dns-pihole-unbound/#troubleshooting","title":"\ud83e\uddf0 Troubleshooting","text":""},{"location":"infra/dns-pihole-unbound/#connection-refused","title":"\u274c <code>connection refused</code>","text":"<p>Cause: Pi-hole not running in host networking OR port 53 in use.</p> <p>Fix:</p> <ul> <li>Ensure <code>network_mode: host</code> is present</li> <li>Restart Pi-hole</li> </ul>"},{"location":"infra/dns-pihole-unbound/#docker-proxy-appears-in-ss-output","title":"\u274c <code>docker-proxy</code> appears in ss output","text":"<p>Cause: Pi-hole using <code>ports:</code> mapping instead of host networking.</p> <p>Fix:</p> <ul> <li>Remove ALL <code>ports:</code> from Pi-hole config</li> <li>Redeploy containers</li> </ul>"},{"location":"infra/dns-pihole-unbound/#unbound-not-responding","title":"\u274c Unbound not responding","text":"<p>Check port:</p> <pre><code>dig @127.0.0.1 -p 5335 google.com\n</code></pre> <p>If no response:</p> <ul> <li>Unbound container not ready</li> <li>Port blocked</li> <li>Container crashed</li> </ul>"},{"location":"infra/dns-pihole-unbound/#pi-hole-not-resolving-lan-hostnames","title":"\u274c Pi-hole not resolving LAN hostnames","text":"<p>Enable Conditional Forwarding OR add host records in:</p> <pre><code>/etc/pihole/custom.list\n</code></pre>"},{"location":"infra/dns-pihole-unbound/#summary","title":"\ud83e\udded Summary","text":"<p>You now have:</p> <ul> <li>Full DNS privacy</li> <li>No 3rd-party upstream DNS</li> <li>Root-level recursion</li> <li>Fast caching</li> <li>Pi-hole running natively at host network speed</li> </ul> <p>A clean, secure, resilient Lodge DNS system.</p> <p>If you'd like an advanced version of this doc with:</p> <p>\ud83d\udd27 Hardening (DNSSEC, QNAME minimization, root anchors) \ud83d\udcca Grafana dashboards \ud83d\udd25 High availability (2nd Pi-hole) \ud83d\udce6 GitHub-ready SOP version \ud83d\udd3a Twin Peaks\u2013themed version</p> <p>Just say the word.</p>"},{"location":"infra/network-baseline/","title":"Network Baseline","text":""},{"location":"infra/network-baseline/#network-baseline-pi-hole-1-configuration-operational-as-of-nov-16-2025","title":"Network Baseline: Pi-hole 1 Configuration (Operational as of Nov 16, 2025)","text":""},{"location":"infra/network-baseline/#overview","title":"\ud83e\udde9 Overview","text":"<p>Pi-hole 1 (192.168.199.53) is now operating as the primary DNS server for the network. The secondary DNS has been left as Cloudflare (1.1.1.1) temporarily for redundancy during observation.</p>"},{"location":"infra/network-baseline/#router-dhcp-configuration-tp-link","title":"\ud83c\udf10 Router DHCP Configuration (TP-Link)","text":"<p>DHCP Server: Enabled IP Address Pool: <code>192.168.199.99 \u2013 192.168.199.201</code> Address Lease Time: 120 minutes Default Gateway: <code>192.168.199.1</code> Primary DNS: <code>192.168.199.53</code> (Pi-hole 1) Secondary DNS: <code>1.1.1.1</code> (Cloudflare, temporary fallback)</p>"},{"location":"infra/network-baseline/#pi-hole-1-details","title":"\ud83e\udde0 Pi-hole 1 Details","text":"<ul> <li>Status: Running normally</li> <li>IP Address: <code>192.168.199.53</code></li> <li>Role: Primary DNS server for LAN</li> <li>Container health: Healthy (FTL process active)</li> <li>DNS test results: Successful resolution via <code>nslookup google.com 192.168.199.53</code></li> </ul>"},{"location":"infra/network-baseline/#monitoring-instructions-48-hour-observation","title":"\ud83d\udd75\ufe0f Monitoring Instructions (48-hour observation)","text":"<ol> <li>UI \u2192 Query Log: Confirm steady traffic and minimal \u201cretried\u201d queries.</li> <li>UI \u2192 Tools \u2192 Network: Verify active client list is populating.</li> <li>From any LAN host:</li> </ol> <pre><code>nslookup cloudflare.com 192.168.199.53\n</code></pre> <ul> <li>Should return valid IPv4/IPv6 results quickly.</li> <li>If browsing stalls:</li> </ul> <pre><code>pihole -t\n</code></pre> <ul> <li>Use this to tail real-time logs and confirm query flow.</li> </ul>"},{"location":"infra/network-baseline/#next-steps-after-stability-confirmed","title":"\ud83e\udeb6 Next Steps (after stability confirmed)","text":"<p>Once Pi-hole 1 proves stable:</p> <ul> <li>Deploy new Unbound server on a clean VM (not <code>.139</code>).</li> <li>Integrate Pi-hole 1 with that Unbound for recursive resolution.</li> <li>Remove Cloudflare fallback (1.1.1.1) to enforce full DNS filtering.</li> </ul>"},{"location":"infra/network-baseline/#next-change-procedure-switching-to-recursive-unbound","title":"\u2699\ufe0f Next Change Procedure: Switching to Recursive Unbound","text":"<ol> <li>Deploy Unbound Container:</li> </ol> <pre><code>docker run -d --name unbound \\\n  -p 5335:5335/tcp \\\n  -p 5335:5335/udp \\\n  -v /opt/unbound/etc:/etc/unbound \\\n  --restart unless-stopped \\\n  mvance/unbound:latest\n</code></pre> <ol> <li>Update Pi-hole Upstream DNS in UI:    Go to Settings \u2192 DNS \u2192 Upstream DNS Servers and set:</li> </ol> <pre><code>Custom 1 (IPv4): 127.0.0.1#5335\n</code></pre> <ol> <li> <p>Disable Cloudflare Fallback:    Remove <code>1.1.1.1</code> from your router\u2019s DHCP config.</p> </li> <li> <p>Verify with dig/nslookup:</p> </li> </ol> <pre><code>dig google.com @127.0.0.1 -p 5335\n</code></pre> <p>If that resolves, Unbound is active.</p> <ol> <li>Reboot Clients or Flush DNS:</li> </ol> <pre><code>ipconfig /flushdns   # Windows\nsudo systemd-resolve --flush-caches  # Linux\n</code></pre> <ol> <li>Monitor Logs:</li> </ol> <pre><code>docker logs -f unbound\npihole -t\n</code></pre> <p>Once Unbound has stable recursion, log the new baseline as Phase 2: Recursive DNS.</p>"},{"location":"infra/network-baseline/#current-state-summary","title":"\u2705 Current State Summary","text":"Component IP Address Role Status TP-Link Router 192.168.199.1 Gateway &amp; DHCP Active Pi-hole 1 192.168.199.53 Primary DNS Operational Cloudflare 1.1.1.1 Secondary DNS (Fallback) Enabled <p>Documented by Agent Osiris \u2013 maintained in the Lodge Systems Logbook</p>"},{"location":"infra/network-diagram/","title":"Network Diagram","text":"EVIDENCE <p>nav:   - Home: index.md</p> <ul> <li> <p>Infrastructure:</p> <ul> <li>Overview: infra/overview.md</li> <li>Network Diagram: infra/network-diagram.md</li> <li>Storage &amp; Backups: infra/storage-backups.md</li> <li>Automation &amp; Workflows: infra/automation.md</li> </ul> </li> <li> <p>Servers:</p> <ul> <li>Pacard Sawmill: servers/pacard-sawmill.md</li> <li>Gordon Cole: servers/gordon-cole.md</li> <li>Bookhouse: servers/bookhouse.md</li> <li>Blockbuster: servers/blockbuster.md</li> <li>RARR (Media Server): servers/rarr.md</li> <li>DNS Gateway (VM 198): servers/dns-gateway.md</li> <li>The White Lodge (PVE Node): servers/thewhitelodge.md</li> <li>The Black Lodge (PVE Node): servers/theblacklodge.md</li> <li>The Red Room (PVE Node / PBS): servers/theredroom.md</li> </ul> </li> <li> <p>Applications:</p> <ul> <li>Media:<ul> <li>Jellyfin: apps/jellyfin.md</li> <li>Radarr: apps/radarr.md</li> <li>Sonarr: apps/sonarr.md</li> <li>Prowlarr: apps/prowlarr.md</li> <li>qBittorrent (VPN): apps/qbittorrent.md</li> <li>Xteve / TVHeadend: apps/xteve-tvheadend.md</li> </ul> </li> <li>Books:<ul> <li>Readarr: apps/readarr.md</li> <li>Kavita: apps/kavita.md</li> <li>Audiobookshelf: apps/audiobookshelf.md</li> </ul> </li> <li>Networking &amp; DNS:<ul> <li>Pi-hole: apps/pihole.md</li> <li>Unbound: apps/unbound.md</li> <li>WireGuard: apps/wireguard.md</li> </ul> </li> <li>Infrastructure Apps:<ul> <li>Portainer: apps/portainer.md</li> <li>Nginx Proxy Manager: apps/npm.md</li> <li>Homepage Dashboard: apps/homepage.md</li> <li>Flame Dashboard: apps/flame.md</li> </ul> </li> </ul> </li> <li> <p>Cloudflare:</p> <ul> <li>Tunnel for Code-Server: cloudflare/cloudflare-tunnel-code-server.md</li> <li>Future Tunnels: cloudflare/tunnels.md</li> </ul> </li> <li> <p>Guides:</p> <ul> <li>Container Deployment SOP: guides/container-deploy.md</li> <li>VM Creation SOP: guides/vm-create.md</li> <li>Network Troubleshooting: guides/network-troubleshoot.md</li> <li>Backup &amp; Restore SOP: guides/backup-restore.md</li> </ul> </li> </ul> <p>Important: Mermaid diagrams only work inside triple backticks ```mermaid</p>"},{"location":"infra/network-diagram/#3-make-sure-its-in-the-navigation-mkdocsyml","title":"\u2705 3. Make sure it\u2019s in the navigation (mkdocs.yml)","text":"<p>You already have:</p> <p>```yaml   - Infrastructure:       - Overview: infra/overview.md       - Network Diagram: infra/network-diagram.md</p>"},{"location":"infra/overview/","title":"Infrastructure Overview","text":"<p>The infrastructure layer forms the foundation of the homelab. It includes the network, DNS, storage, virtualization stack, and all underlying systems that support servers, services, and automation. This section documents how the environment is structured, how core components operate, and what must be restored first when issues arise.</p>"},{"location":"infra/overview/#purpose-of-this-section","title":"Purpose of This Section","text":"<p>This section exists to answer questions such as:</p> <ul> <li>How is the homelab physically and logically structured?</li> <li>Where do DNS, routing, and VLANs live?</li> <li>How is storage arranged and protected?</li> <li>What does the high-level network look like?</li> <li>What components are foundational for recovery or troubleshooting?</li> </ul> <p>It provides the technical backbone for the entire environment \u2014 a clear reference for configuration, troubleshooting, and future expansion.</p>"},{"location":"infra/overview/#what-the-infrastructure-covers","title":"What the Infrastructure Covers","text":"<p>Infrastructure includes:</p> <ul> <li>Physical and virtual networks  </li> <li>DNS and name resolution  </li> <li>Storage systems and ZFS  </li> <li>Proxmox hypervisors and virtualization  </li> <li>Supporting operational tools  </li> </ul> <p>Each subsection goes deeper into the technical design of the homelab.</p>"},{"location":"infra/overview/#network","title":"Network","text":"<p>The network defines how devices communicate and how traffic is secured.</p> <p>Documented here:</p> <ul> <li>Network Diagram \u2013 Logical layout of routers, switches, and VLANs  </li> <li>Network Baseline \u2013 IP schema, routing, firewall concepts  </li> </ul> <p>\u27a1\ufe0f See the Network subsection for details.</p>"},{"location":"infra/overview/#dns","title":"DNS","text":"<p>Reliable name resolution supports internal services, certificates, and automation.</p> <p>This section covers:</p> <ul> <li>Pi-hole + Unbound  </li> <li>DNS filtering  </li> <li>Local domain handling  </li> <li>Upstream resolver behavior  </li> </ul> <p>\u27a1\ufe0f See DNS for configuration and troubleshooting.</p>"},{"location":"infra/overview/#storage","title":"Storage","text":"<p>Storage underpins media, VMs, backups, and persistent data.</p> <p>Topics include:</p> <ul> <li>Dataset layout  </li> <li>Media directories  </li> <li>NFS/SMB (if applicable)  </li> <li>Integration with Proxmox  </li> </ul> <p>\u27a1\ufe0f See Storage for full details.</p>"},{"location":"infra/overview/#zfs","title":"ZFS","text":"<p>ZFS provides data integrity, checksumming, snapshots, and replication.</p> <p>Covered here:</p> <ul> <li>Pool layout  </li> <li>Dataset structure  </li> <li>Scrub scheduling  </li> <li>Snapshot strategy  </li> <li>Replication strategies  </li> </ul> <p>\u27a1\ufe0f See ZFS for operational details.</p>"},{"location":"infra/overview/#virtualization-layer","title":"Virtualization Layer","text":"<p>The Proxmox hypervisors host the majority of the homelab workloads.</p> <p>This section will cover:</p> <ul> <li>Node responsibilities  </li> <li>VM layout  </li> <li>Container layout  </li> <li>Backup/restore workflows  </li> <li>Migration or failover considerations  </li> </ul> <p>Last updated: TODO-SET-DATE</p>"},{"location":"infra/storage/","title":"Storage","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"infra/virtualization/","title":"Virtualization Overview (Proxmox)","text":"<p>The virtualization layer is the core platform on which nearly all homelab workloads run. Proxmox VE provides the hypervisor, clustering, storage integration, snapshots, and management interfaces used to deploy and maintain virtual machines and containers.</p> <p>This page serves as the high-level reference for how virtualization is structured in the environment.</p>"},{"location":"infra/virtualization/#goals-of-the-virtualization-layer","title":"Goals of the Virtualization Layer","text":"<ul> <li>Centralize compute resources  </li> <li>Isolate workloads into VMs and containers  </li> <li>Provide simple backup and restore options  </li> <li>Enable testing and experimentation safely  </li> <li>Support future expansion and additional nodes  </li> </ul>"},{"location":"infra/virtualization/#current-proxmox-nodes","title":"Current Proxmox Nodes","text":"<p>List your nodes here (example):</p> <ul> <li>The White Lodge \u2013 Primary hypervisor  </li> <li>The Black Lodge \u2013 Secondary hypervisor, backup target  </li> <li>Pacard Sawmill \u2013 Utility compute / dev workloads  </li> </ul> <p>Each node typically includes:</p> <ul> <li>CPU, memory, and storage layout  </li> <li>Network bridges (e.g., vmbr0, vmbr1)  </li> <li>Attached storage pools (ZFS, NFS, or local-lvm)  </li> </ul>"},{"location":"infra/virtualization/#virtual-machines-vms","title":"Virtual Machines (VMs)","text":"<p>Document key VMs:</p> <ul> <li>Purpose  </li> <li>OS type  </li> <li>Disks &amp; storage  </li> <li>Backup strategy  </li> <li>Dependencies  </li> <li>Snapshots (automatic or manual)</li> </ul> <p>Examples:</p> <ul> <li>RARR VM \u2014 Media automation  </li> <li>DNS / Pi-hole VM \u2014 DNS core  </li> <li>Utility VM \u2014 Code-server, docker, tooling  </li> </ul>"},{"location":"infra/virtualization/#linux-containers-lxc","title":"Linux Containers (LXC)","text":"<p>If you use LXC, list:</p> <ul> <li>Purpose  </li> <li>Network mode  </li> <li>Backing storage  </li> <li>Security considerations  </li> <li>Backup configuration  </li> </ul>"},{"location":"infra/virtualization/#storage-integration","title":"Storage Integration","text":"<p>Proxmox ties into ZFS and other storage layers.</p> <p>Document:</p> <ul> <li>Pools assigned to each node  </li> <li>ZFS datasets used for VM disks  </li> <li>Any replication schedules  </li> </ul> <p>Examples:</p> <ul> <li><code>mypool/vmdata</code> \u2192 VM disks  </li> <li><code>mypool/containers</code> \u2192 LXC rootfs  </li> <li>ZFS replication \u2192 The Black Lodge nightly  </li> </ul>"},{"location":"infra/virtualization/#backups","title":"Backups","text":"<p>Document backup behavior:</p> <ul> <li>Proxmox Backup Server (PBS) or external storage  </li> <li>Backup schedules  </li> <li>Retention policies  </li> <li>How to restore a VM or container  </li> </ul> <p>Examples:</p> <ul> <li>Daily backups at 2 AM  </li> <li>Retention: 7 daily, 4 weekly, 1 monthly  </li> <li>Replicated snapshots to secondary node  </li> </ul>"},{"location":"infra/virtualization/#networking-in-proxmox","title":"Networking in Proxmox","text":"<p>Explain:</p> <ul> <li>Bridges (vmbr0 / vmbr1)  </li> <li>VLANs  </li> <li>Trunk ports  </li> <li>Network segregation for lab, media, IoT, guest networks  </li> </ul>"},{"location":"infra/virtualization/#failover-recovery","title":"Failover &amp; Recovery","text":"<p>Document any manual or automatic failover procedures:</p> <ul> <li>ZFS replication to backup node  </li> <li>Restoring a VM on a different node  </li> <li>Floating IPs or planned HA  </li> </ul> <p>Last updated: TODO-SET-DATE</p>"},{"location":"infra/zfs/","title":"ZFS","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"lodges/","title":"The Lodges","text":"<p>Field directory for each Lodge node and its purpose in the cascade.</p>"},{"location":"lodges/#nodes","title":"Nodes","text":"<ul> <li>Packard Sawmill</li> <li>The White Lodge</li> <li>The Black Lodge</li> <li>The Owl Cave (NAS)</li> <li>The Bookhouse (Readarr VM)</li> <li>Gordon Cole Server</li> <li>RARR (Main Stack)</li> </ul>"},{"location":"lodges/bookhouse/","title":"Bookhouse","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"lodges/gordon-cole/","title":"Gordon Cole","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"lodges/owlcave/","title":"Owl Cave","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"lodges/rarr/","title":"RARR Lodge","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"ops/","title":"Operations","text":"<p>Daily procedures, tunnel work, failover plans, and other living runbooks.</p>"},{"location":"ops/#runbooks","title":"Runbooks","text":"<ul> <li>Cloudflare Operations</li> <li>Code-Server Tunnel</li> <li>VPN Stack</li> <li>Automation</li> <li>Failover Cascade Model</li> <li>Jellyfin Infrastructure</li> </ul>"},{"location":"ops/automation/","title":"Automation","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"ops/cascade/","title":"Cascade","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"ops/cloudflare/","title":"Cloudflare","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"ops/codeserver/","title":"Codeserver","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"ops/jellyfin/","title":"Jellyfin","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"ops/vpn/","title":"Vpn","text":"<p>Work in progress. Notes coming soon.</p>"},{"location":"redroom/vscode-twinpeaks-theme/","title":"Twin Peaks VS Code Theme &amp; Icon Pack (VSIX)","text":""},{"location":"redroom/vscode-twinpeaks-theme/#overview","title":"Overview","text":"<p>This guide documents the full process of creating, packaging, and publishing the Twin Peaks VS Code Theme Pack, including the Red Room color theme, Lodge Icon Pack, and a VSIX installer so it can be used across machines or shared publicly.</p> <p>This extension includes: - Twin Peaks: Red Room (dark editor theme) - Lodge Icon Pack (custom VS Code file/folder icons) - Complete VSIX package - GitHub repository setup - Optional marketplace publishing</p>"},{"location":"redroom/vscode-twinpeaks-theme/#install-the-twin-peaks-vsix","title":"Install the Twin Peaks VSIX","text":""},{"location":"redroom/vscode-twinpeaks-theme/#1-upload-or-place-the-vsix-file","title":"1. Upload or place the VSIX file","text":"<p>Place the file:</p> <pre><code>twinpeaks-theme-pack-1.0.0.vsix\n</code></pre> <p>somewhere easy to access, such as:</p> <pre><code>~/projects/vscode-twinpeaks/\n</code></pre>"},{"location":"redroom/vscode-twinpeaks-theme/#2-install-in-vs-code-desktop","title":"2. Install in VS Code (Desktop)","text":"<p>Open VS Code \u2192 Extensions sidebar \u2192 Click \u22ee (three dots) \u2192 Install from VSIX\u2026 \u2192 select the file.</p>"},{"location":"redroom/vscode-twinpeaks-theme/#3-install-in-code-server","title":"3. Install in code-server","text":"<p>Use:</p> <pre><code>code-server --install-extension twinpeaks-theme-pack-1.0.0.vsix\n</code></pre> <p>Reload the browser tab.</p>"},{"location":"redroom/vscode-twinpeaks-theme/#4-activate-the-theme-icons","title":"4. Activate the theme &amp; icons","text":"<p>Press F1 \u2192 select:</p> <ul> <li>Preferences: Color Theme \u2192 Twin Peaks: Red Room</li> <li>Preferences: File Icon Theme \u2192 Twin Peaks: Lodge Icons</li> </ul>"},{"location":"redroom/vscode-twinpeaks-theme/#repository-setup-github","title":"Repository Setup (GitHub)","text":""},{"location":"redroom/vscode-twinpeaks-theme/#1-install-github-cli","title":"1. Install GitHub CLI","text":"<pre><code>sudo apt install gh -y\n</code></pre>"},{"location":"redroom/vscode-twinpeaks-theme/#2-authenticate-github-cli","title":"2. Authenticate GitHub CLI","text":"<pre><code>gh auth login\n</code></pre> <p>Choose: - GitHub.com - HTTPS - Yes to git credential login - Authenticate with browser  </p>"},{"location":"redroom/vscode-twinpeaks-theme/#3-create-the-repository","title":"3. Create the repository","text":"<p>From inside the project folder:</p> <pre><code>cd ~/projects/vscode-twinpeaks\ngh repo create vscode-twinpeaks --public --source=. --remote=origin --push\n</code></pre> <p>Your repo is now live on GitHub.</p>"},{"location":"redroom/vscode-twinpeaks-theme/#mkdocs-navigation-entry","title":"MkDocs Navigation Entry","text":"<p>Add this to <code>mkdocs.yml</code> under your desired section:</p> <pre><code>  - Twin Peaks VS Code Theme Pack: vscode-twinpeaks.md\n</code></pre>"},{"location":"redroom/vscode-twinpeaks-theme/#changelog","title":"Changelog","text":""},{"location":"redroom/vscode-twinpeaks-theme/#v100-initial-release","title":"v1.0.0 \u2013 Initial Release","text":"<ul> <li>Added Red Room VS Code color theme  </li> <li>Added Lodge Icon Pack (folders + files)  </li> <li>Packaged full extension as VSIX  </li> <li>Set up GitHub repo structure  </li> <li>Documented installation and usage  </li> </ul> <p>Prepared in cooperation with Agent Cooper. The owls are watching.</p>"},{"location":"servers/packard-sawmill/","title":"pacard-sawmill","text":"<p>Role: Development server / code-server host OS: Ubuntu 24.04 LTS (VM) Primary IP: 192.168.199.177 Status: Active</p>"},{"location":"servers/packard-sawmill/#overview","title":"Overview","text":"<p>pacard-sawmill serves as the main development environment for internal and external projects. It runs code-server, Docker, and several MkDocs documentation sites.</p>"},{"location":"servers/packard-sawmill/#key-services","title":"Key Services","text":"<ul> <li>code-server (<code>code.osirispc.com</code> via Cloudflare Tunnel)</li> <li>Portainer Agent (port 9001)</li> <li>Homelab Docs (<code>homelab-docs</code> auto-commit + watcher)</li> <li>Docker-based development containers</li> </ul>"},{"location":"servers/packard-sawmill/#maintenance-notes","title":"Maintenance Notes","text":"<ul> <li>Automatic updates via <code>unattended-upgrades</code></li> <li>Auto-commit logs rotate every 50 lines</li> <li>Watched in real-time by <code>homelab-autocommit.service</code></li> <li>Backups planned for sync to <code>theowlcave</code> NAS</li> </ul>"},{"location":"servers/packard-sawmill/#recent-changes","title":"Recent Changes","text":"<p>Last update: {{ date }}</p>"},{"location":"servers/theblacklodge/","title":"theblacklodge","text":"<p>Role: Secondary Proxmox Node (Backup / mirror media server) OS: Debian 12 (bookworm) Primary IP: 192.168.199.101 Status: Active (standby replication node)</p>"},{"location":"servers/theblacklodge/#overview","title":"Overview","text":"<p>The Black Lodge acts as the failover counterpart to The White Lodge. It mirrors media libraries, configurations, and snapshots via ZFS and rsync. When The White Lodge goes dark, The Black Lodge assumes its identity temporarily.</p>"},{"location":"servers/theblacklodge/#key-services","title":"Key Services","text":"<ul> <li>Jellyfin (standby configuration)</li> <li>RARR stack (backup, disabled by default)</li> <li>Rsync mirror target for <code>/mnt/data/movies</code> and <code>/mnt/data/shows</code></li> <li>Portainer Agent for container control</li> </ul>"},{"location":"servers/theblacklodge/#maintenance-notes","title":"Maintenance Notes","text":"<ul> <li>Receives media and config sync via <code>lodge-mirror.sh</code></li> <li>Keeps ZFS snapshots aligned daily</li> <li>Does not delete missing files (append-only sync)</li> <li>PBS handles local and offsite backups</li> </ul>"},{"location":"servers/theblacklodge/#failover-plan","title":"Failover Plan","text":"<p>During failover, The Black Lodge mounts the mirrored datasets and starts the Jellyfin container. Cloudflare DNS may be updated manually or via Keepalived floating IP for continuity.</p>"},{"location":"servers/theblacklodge/#recent-changes","title":"Recent Changes","text":"<p>Last update: {{ date }}</p>"},{"location":"servers/thewhitelodge/","title":"thewhitelodge","text":"<p>Role: Primary Proxmox Node OS: Debian 12 (bookworm) Primary IP: 192.168.199.100  Status: Active</p>"},{"location":"servers/thewhitelodge/#overview","title":"Overview","text":"<p>The White Lodge hosts several vm's.  The main RARR stack: Radarr, Sonarr, Jellyfin, qBittorrent, and Gluetun.  It serves as the production media hub for the network and the primary sync source for The Black Lodge.</p>"},{"location":"servers/thewhitelodge/#key-services","title":"Key Services","text":"<ul> <li>Jellyfin (Movies, Shows, Trailers)</li> <li>Radarr, Sonarr, Prowlarr, Jellyseerr</li> <li>qBittorrent with VPN via Gluetun</li> <li>ZFS dataset for <code>/mnt/data/movies</code> and <code>/mnt/data/shows</code></li> </ul>"},{"location":"servers/thewhitelodge/#maintenance-notes","title":"Maintenance Notes","text":"<ul> <li>ZFS snapshots replicate to The Black Lodge  </li> <li>Synced nightly using <code>lodge-mirror.sh</code> </li> <li>Backed up via Proxmox Backup Server  </li> <li>Config stored in <code>/mnt/data/configs/</code></li> </ul>"},{"location":"servers/thewhitelodge/#failover-plan","title":"Failover Plan","text":"<p>In the event of downtime, The Black Lodge takes over via manual ZFS replication and Jellyfin restore. Rsync ensures latest metadata and libraries are mirrored.</p>"},{"location":"servers/thewhitelodge/#recent-changes","title":"Recent Changes","text":"<p>Last update: {{ date }}</p>"}]}